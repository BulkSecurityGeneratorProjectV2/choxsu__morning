/*
 Navicat MySQL Data Transfer

 Source Server         : 127.0.0.1
 Source Server Type    : MySQL
 Source Server Version : 80012
 Source Host           : 127.0.0.1:3306
 Source Schema         : sblog

 Target Server Type    : MySQL
 Target Server Version : 80012
 File Encoding         : 65001

 Date: 28/11/2018 08:44:53
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for account
-- ----------------------------
DROP TABLE IF EXISTS `account`;
CREATE TABLE `account`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `nickName` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `userName` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `password` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `salt` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `status` int(11) NOT NULL,
  `createAt` datetime(0) NOT NULL,
  `ip` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `avatar` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `likeCount` int(11) NOT NULL DEFAULT 0 COMMENT '被赞次数',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 6 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of account
-- ----------------------------
INSERT INTO `account` VALUES (1, 'ChoxSu', 'choxsu@gmail.com', '0578028539a72c0bff4ea80510ad79548a89534c03ed376682de1ed457669f9e', '8wCSdUV6EJBvPg9zDkphts9JAHFNyO6t', 1, '2018-04-18 09:00:19', '175.12.244.105', '0/1.jpg', 0);
INSERT INTO `account` VALUES (2, '管理员', 'admin@styg.site', '0578028539a72c0bff4ea80510ad79548a89534c03ed376682de1ed457669f9e', '8wCSdUV6EJBvPg9zDkphts9JAHFNyO6t', 1, '2018-04-19 10:19:11', '175.12.244.105', '0/1.jpg', 0);
INSERT INTO `account` VALUES (3, 'test', 'test@test.com', '237864b239e92ca059cb4cce802712761bd22aef15b9f7eea2dedc5c5000a7ee', '3_ksssmNhcyEx9u5n-tTsu7qumVL0WNG', 1, '2018-09-27 11:30:08', '0:0:0:0:0:0:0:1', 'x.jpg', 0);
INSERT INTO `account` VALUES (4, 'test1', '2283546325@qq.com', 'cb114f83eb7ccbdb4a0e4786be1bac1beee14349ec81b1382a14d86bc6e5a008', 'ZBkrSCUZJGcnaf-ZX3pEUDETjBtAVR2S', 1, '2018-09-27 11:33:19', '0:0:0:0:0:0:0:1', 'x.jpg', 0);
INSERT INTO `account` VALUES (5, 'test2', 'test2@test.com', '193701796388d236cdb90dacf43e418e70d6a4c81f98f69ce5c8b6b67adf667a', 'VW6lmElQKJVgSyhUBIkqKc5EHHzNDBOF', 1, '2018-09-27 11:34:12', '0:0:0:0:0:0:0:1', 'x.jpg', 0);

-- ----------------------------
-- Table structure for account_role
-- ----------------------------
DROP TABLE IF EXISTS `account_role`;
CREATE TABLE `account_role`  (
  `accountId` int(11) NOT NULL,
  `roleId` int(11) NOT NULL,
  PRIMARY KEY (`accountId`, `roleId`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of account_role
-- ----------------------------
INSERT INTO `account_role` VALUES (1, 1);
INSERT INTO `account_role` VALUES (2, 8);
INSERT INTO `account_role` VALUES (3, 9);
INSERT INTO `account_role` VALUES (5, 9);

-- ----------------------------
-- Table structure for auth_code
-- ----------------------------
DROP TABLE IF EXISTS `auth_code`;
CREATE TABLE `auth_code`  (
  `id` varchar(33) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `accountId` int(11) NOT NULL,
  `expireAt` bigint(20) NOT NULL,
  `type` int(20) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Table structure for blog
-- ----------------------------
DROP TABLE IF EXISTS `blog`;
CREATE TABLE `blog`  (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `accountId` int(11) NULL DEFAULT NULL COMMENT '博客主id',
  `title` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '标题',
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '内容',
  `markedContent` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NULL COMMENT '待解析内容',
  `createAt` datetime(0) NOT NULL COMMENT '创建时间',
  `updateAt` datetime(0) NOT NULL COMMENT '修改时间',
  `clickCount` int(11) NOT NULL DEFAULT 0 COMMENT '点击次数',
  `likeCount` int(11) NOT NULL DEFAULT 0 COMMENT '喜欢次数',
  `favoriteCount` int(11) NOT NULL DEFAULT 0 COMMENT '收藏次数',
  `category` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '' COMMENT '类型 note（笔记）favorite(收藏）code(代码）about(关于）',
  `isDelete` int(11) NOT NULL DEFAULT 0 COMMENT '是否删除 0否1是',
  `tag_id` int(11) NULL DEFAULT NULL COMMENT 'tag_id',
  `category_id` int(11) NULL DEFAULT NULL COMMENT '代码分类id，如果category为code时候，这个值才会生效',
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `title`(`title`) USING BTREE,
  INDEX `tagId`(`tag_id`) USING BTREE,
  INDEX `isDel`(`isDelete`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 39 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of blog
-- ----------------------------
INSERT INTO `blog` VALUES (14, 1, '关于我', '<h5 id=\"h5--choxsu\"><a name=\"网名     ChoxSu\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>网名     ChoxSu</h5><h5 id=\"h5--\"><a name=\"爱好     爬山、电影、游戏\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>爱好     爬山、电影、游戏</h5><h5 id=\"h5--\"><a name=\"签名     没有思考，人生的路会越走越难！\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>签名     没有思考，人生的路会越走越难！</h5><h5 id=\"h5--\"><a name=\"地点     重庆\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>地点     重庆</h5><h5 id=\"h5--\"><a name=\"性别     男\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>性别     男</h5><h6 id=\"h6--\"><a name=\"博客地址：   记得点我看你想看的博客\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>博客地址： <a href=\"http://www.styg.site\">记得点我看你想看的博客</a></h6><h6 id=\"h6--\"><a name=\"我的博客，重点是记录我的技术总结，让更多人解决问题，学习知识，没有批评就没有进步。\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>我的博客，重点是记录我的技术总结，让更多人解决问题，学习知识，没有批评就没有进步。</h6><hr>\n<h5 id=\"h5--mysql-ssh-\"><a name=\"已经更新了两篇博客一个关于mysql ， 一个是ssh远程登录的博客，都是工作时遇到的都记录在了这里\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>已经更新了两篇博客一个关于mysql ， 一个是ssh远程登录的博客，都是工作时遇到的都记录在了这里</h5><hr>\n<p>之前域名已更改为主域名，不在以二级域名来访问，www.styg.site  </p>\n', '##### 网名     ChoxSu\n##### 爱好     爬山、电影、游戏\n##### 签名     没有思考，人生的路会越走越难！\n##### 地点     重庆\n##### 性别     男\n###### 博客地址： [记得点我看你想看的博客](http://www.styg.site)\n\n###### 我的博客，重点是记录我的技术总结，让更多人解决问题，学习知识，没有批评就没有进步。\n\n------------\n\n##### 已经更新了两篇博客一个关于mysql ， 一个是ssh远程登录的博客，都是工作时遇到的都记录在了这里  \n\n----\n之前域名已更改为主域名，不在以二级域名来访问，www.styg.site  ', '2018-07-16 12:18:18', '2018-06-13 16:50:27', 58, 0, 0, 'about', 0, NULL, NULL);
INSERT INTO `blog` VALUES (15, 1, 'JFinal ActiveRecordPlugin插件事物交给Spring管理', '<p>  最近在SpringBoot中使用JFinal的ActiveRecordPlugin插件，虽然事物可以直接通过<a href=\"https://github.com/Before\" title=\"&#64;Before\" class=\"at-link\">@Before</a>(Tx.class)来解决，但是后面项目的需要将事物交给spring来管理，具体实现看下去</p>\n<h3 id=\"h3--spring-aop-\"><a name=\"思路：使用spring AOP代理\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>思路：使用spring AOP代理</h3><p>这里使用springboot来实现，spring同理</p>\n<h4 id=\"h4-maven-\"><a name=\"maven 依赖\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>maven 依赖</h4><pre><code>        &lt;dependency&gt;&lt;!-- spring boot aop starter依赖 --&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 数据源 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;\n            &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;\n        &lt;/dependency&gt;     \n      其他的依赖直接省略了，做程序的都知道的\n</code></pre><h6 id=\"h6--jfinal\"><a name=\"感谢   如梦技术的代码片段  ,   JFinal\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>感谢 <a href=\"https://gitee.com/596392912/codes\">如梦技术的代码片段</a> , <a href=\"http://jfinal.com\">JFinal</a></h6><h4 id=\"h4-jfinaltxaop\"><a name=\"JFinalTxAop\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>JFinalTxAop</h4><pre><code>package com.choxsu.elastic.config;\n\nimport com.jfinal.kit.LogKit;\nimport com.jfinal.plugin.activerecord.ActiveRecordException;\nimport com.jfinal.plugin.activerecord.Config;\nimport com.jfinal.plugin.activerecord.DbKit;\nimport com.jfinal.plugin.activerecord.NestedTransactionHelpException;\nimport com.jfinal.plugin.activerecord.tx.TxConfig;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.aspectj.lang.reflect.MethodSignature;\nimport org.springframework.stereotype.Component;\n\nimport java.lang.reflect.Method;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\n/**\n * @author choxsu\n * @date 2018/4/13\n */\n@Aspect\n@Component\npublic class JFinalTxAop {\n\n\n    /**\n     * 自定义JFinal 事物注解\n     * value中的意思解释\n     *\n     * @annotation 表示注解只能支持方法上\n     * @within 表示注解在类下面所有的方法 ， 暂时不使用这种方式\n     */\n    @Pointcut(&quot;@annotation(com.choxsu.elastic.config.JFinalTx)&quot;)\n    private void method() {\n    }\n\n    @Around(value = &quot;method()&quot;, argNames = &quot;pjp&quot;)\n    public Object doAround(ProceedingJoinPoint pjp) throws Throwable {\n        Object retVal = null;\n        Config config = getConfigWithTxConfig(pjp);\n        if (config == null)\n            config = DbKit.getConfig();\n\n        Connection conn = config.getThreadLocalConnection();\n        // Nested transaction support\n        if (conn != null) {\n            try {\n                if (conn.getTransactionIsolation() &lt; getTransactionLevel(config))\n                    conn.setTransactionIsolation(getTransactionLevel(config));\n                retVal = pjp.proceed();\n                return retVal;\n            } catch (SQLException e) {\n                throw new ActiveRecordException(e);\n            }\n        }\n\n        Boolean autoCommit = null;\n        try {\n            conn = config.getConnection();\n            autoCommit = conn.getAutoCommit();\n            config.setThreadLocalConnection(conn);\n            conn.setTransactionIsolation(getTransactionLevel(config));// conn.setTransactionIsolation(transactionLevel);\n\n            conn.setAutoCommit(false);\n            retVal = pjp.proceed();\n            conn.commit();\n        } catch (NestedTransactionHelpException e) {\n            if (conn != null) try {\n                conn.rollback();\n            } catch (Exception e1) {\n                LogKit.error(e1.getMessage(), e1);\n            }\n            LogKit.logNothing(e);\n        } catch (Throwable t) {\n            if (conn != null) try {\n                conn.rollback();\n            } catch (Exception e1) {\n                LogKit.error(e1.getMessage(), e1);\n            }\n            throw t instanceof RuntimeException ? (RuntimeException) t : new ActiveRecordException(t);\n        } finally {\n            try {\n                if (conn != null) {\n                    if (autoCommit != null)\n                        conn.setAutoCommit(autoCommit);\n                    conn.close();\n                }\n            } catch (Throwable t) {\n                // can not throw exception here, otherwise the more important exception in previous catch block can not be thrown\n                LogKit.error(t.getMessage(), t);\n            } finally {\n                // prevent memory leak\n                config.removeThreadLocalConnection();\n            }\n        }\n        return retVal;\n    }\n\n    /**\n     * 获取配置的事务级别\n     *\n     * @param config\n     * @return\n     */\n    protected int getTransactionLevel(Config config) {\n        return config.getTransactionLevel();\n    }\n\n    /**\n     * @param pjp\n     * @return Config\n     */\n    public static Config getConfigWithTxConfig(ProceedingJoinPoint pjp) {\n        MethodSignature ms = (MethodSignature) pjp.getSignature();\n        Method method = ms.getMethod();\n        TxConfig txConfig = method.getAnnotation(TxConfig.class);\n        if (txConfig == null)\n            txConfig = pjp.getTarget().getClass().getAnnotation(TxConfig.class);\n\n        if (txConfig != null) {\n            Config config = DbKit.getConfig(txConfig.value());\n            if (config == null)\n                throw new RuntimeException(&quot;Config not found with TxConfig: &quot; + txConfig.value());\n            return config;\n        }\n        return null;\n    }\n}\n</code></pre><h4 id=\"h4-jfinaltx\"><a name=\"JFinalTx\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>JFinalTx</h4><pre><code>package com.choxsu.elastic.config;\n\n/**\n * @author choxsu\n */\n\nimport java.lang.annotation.*;\n\n/**\n * Jfinal事物交给spring管理注解\n * 目前只支持用在方法上\n */\n@Inherited\n@Target({ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface JFinalTx {\n\n}\n</code></pre><h3 id=\"h3-u4F7Fu7528\"><a name=\"使用\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>使用</h3><h4 id=\"h4-testcontroller\"><a name=\"TestController\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>TestController</h4><pre><code>package com.choxsu.elastic.controller;\n\nimport com.choxsu.elastic.service.TestService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n/**\n * @author choxsu\n */\n@RestController\n@RequestMapping(value = {&quot;/test/v1&quot;})\npublic class TestController {\n\n\n    @Autowired\n    private TestService testService;\n\n    @GetMapping(value = &quot;/testTran&quot;)\n    public Object testTran(){\n\n\n        return testService.testTran();\n    }\n}\n</code></pre><h4 id=\"h4-testservice\"><a name=\"TestService\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>TestService</h4><pre><code>package com.choxsu.elastic.service;\n\nimport com.choxsu.elastic.config.JFinalTx;\nimport com.jfinal.kit.Ret;\nimport com.jfinal.plugin.activerecord.Db;\nimport com.jfinal.plugin.activerecord.Record;\nimport org.springframework.stereotype.Service;\n\n\n/**\n * @author choxsu\n */\n@Service\npublic class TestService {\n\n    /**\n     * 事物测试\n     *\n     * @return\n     */\n    @JFinalTx\n    public Object testTran() {\n        Record record = new Record();\n        record.set(&quot;id&quot;, 10);\n        Db.save(&quot;test&quot;, record);\n        if (true) {\n            throw new RuntimeException(&quot;test&quot;);\n        }\n        return Ret.by(&quot;msg&quot;, &quot;success&quot;);\n    }\n}\n</code></pre><h5 id=\"h5-sql-\"><a name=\"sql执行了\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>sql执行了</h5><pre><code>Sql: insert into `test`(`id`) values(?)\n</code></pre><h5 id=\"h5-u4F46u662Fu6570u636Eu5E93u6CA1u6709u6570u636E\"><a name=\"但是数据库没有数据\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>但是数据库没有数据</h5><p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-cb026e4ac9652bef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image.png\"></p>\n<p>到此证明事物拦截成功，可以使用spring来管理ActiveRecordPlugin的事物了</p>\n<h4 id=\"h4--code-throw-new-runtimeexception-quot-test-quot-code-\"><a name=\"去掉<code>throw new RuntimeException(&quot;test&quot;);</code>的效果\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>去掉<code>throw new RuntimeException(&quot;test&quot;);</code>的效果</h4><h5 id=\"h5-sql\"><a name=\"sql\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>sql</h5><pre><code>Sql: insert into `test`(`id`) values(?)\n</code></pre><h5 id=\"h5-u6570u636Eu5E93u7ED3u679C\"><a name=\"数据库结果\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>数据库结果</h5><p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-afcb50b9767dbadc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image.png\"></p>\n', '  最近在SpringBoot中使用JFinal的ActiveRecordPlugin插件，虽然事物可以直接通过@Before(Tx.class)来解决，但是后面项目的需要将事物交给spring来管理，具体实现看下去\n### 思路：使用spring AOP代理\n这里使用springboot来实现，spring同理\n\n\n#### maven 依赖\n```\n        <dependency><!-- spring boot aop starter依赖 -->\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-aop</artifactId>\n        </dependency>\n        <!-- 数据源 -->\n        <dependency>\n            <groupId>com.zaxxer</groupId>\n            <artifactId>HikariCP</artifactId>\n        </dependency>     \n      其他的依赖直接省略了，做程序的都知道的  \n```\n###### 感谢 [如梦技术的代码片段](https://gitee.com/596392912/codes) , [JFinal](http://jfinal.com)\n#### JFinalTxAop \n```\npackage com.choxsu.elastic.config;\n\nimport com.jfinal.kit.LogKit;\nimport com.jfinal.plugin.activerecord.ActiveRecordException;\nimport com.jfinal.plugin.activerecord.Config;\nimport com.jfinal.plugin.activerecord.DbKit;\nimport com.jfinal.plugin.activerecord.NestedTransactionHelpException;\nimport com.jfinal.plugin.activerecord.tx.TxConfig;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.aspectj.lang.reflect.MethodSignature;\nimport org.springframework.stereotype.Component;\n\nimport java.lang.reflect.Method;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\n/**\n * @author choxsu\n * @date 2018/4/13\n */\n@Aspect\n@Component\npublic class JFinalTxAop {\n\n\n    /**\n     * 自定义JFinal 事物注解\n     * value中的意思解释\n     *\n     * @annotation 表示注解只能支持方法上\n     * @within 表示注解在类下面所有的方法 ， 暂时不使用这种方式\n     */\n    @Pointcut(\"@annotation(com.choxsu.elastic.config.JFinalTx)\")\n    private void method() {\n    }\n\n    @Around(value = \"method()\", argNames = \"pjp\")\n    public Object doAround(ProceedingJoinPoint pjp) throws Throwable {\n        Object retVal = null;\n        Config config = getConfigWithTxConfig(pjp);\n        if (config == null)\n            config = DbKit.getConfig();\n\n        Connection conn = config.getThreadLocalConnection();\n        // Nested transaction support\n        if (conn != null) {\n            try {\n                if (conn.getTransactionIsolation() < getTransactionLevel(config))\n                    conn.setTransactionIsolation(getTransactionLevel(config));\n                retVal = pjp.proceed();\n                return retVal;\n            } catch (SQLException e) {\n                throw new ActiveRecordException(e);\n            }\n        }\n\n        Boolean autoCommit = null;\n        try {\n            conn = config.getConnection();\n            autoCommit = conn.getAutoCommit();\n            config.setThreadLocalConnection(conn);\n            conn.setTransactionIsolation(getTransactionLevel(config));// conn.setTransactionIsolation(transactionLevel);\n\n            conn.setAutoCommit(false);\n            retVal = pjp.proceed();\n            conn.commit();\n        } catch (NestedTransactionHelpException e) {\n            if (conn != null) try {\n                conn.rollback();\n            } catch (Exception e1) {\n                LogKit.error(e1.getMessage(), e1);\n            }\n            LogKit.logNothing(e);\n        } catch (Throwable t) {\n            if (conn != null) try {\n                conn.rollback();\n            } catch (Exception e1) {\n                LogKit.error(e1.getMessage(), e1);\n            }\n            throw t instanceof RuntimeException ? (RuntimeException) t : new ActiveRecordException(t);\n        } finally {\n            try {\n                if (conn != null) {\n                    if (autoCommit != null)\n                        conn.setAutoCommit(autoCommit);\n                    conn.close();\n                }\n            } catch (Throwable t) {\n                // can not throw exception here, otherwise the more important exception in previous catch block can not be thrown\n                LogKit.error(t.getMessage(), t);\n            } finally {\n                // prevent memory leak\n                config.removeThreadLocalConnection();\n            }\n        }\n        return retVal;\n    }\n\n    /**\n     * 获取配置的事务级别\n     *\n     * @param config\n     * @return\n     */\n    protected int getTransactionLevel(Config config) {\n        return config.getTransactionLevel();\n    }\n\n    /**\n     * @param pjp\n     * @return Config\n     */\n    public static Config getConfigWithTxConfig(ProceedingJoinPoint pjp) {\n        MethodSignature ms = (MethodSignature) pjp.getSignature();\n        Method method = ms.getMethod();\n        TxConfig txConfig = method.getAnnotation(TxConfig.class);\n        if (txConfig == null)\n            txConfig = pjp.getTarget().getClass().getAnnotation(TxConfig.class);\n\n        if (txConfig != null) {\n            Config config = DbKit.getConfig(txConfig.value());\n            if (config == null)\n                throw new RuntimeException(\"Config not found with TxConfig: \" + txConfig.value());\n            return config;\n        }\n        return null;\n    }\n}\n```\n#### JFinalTx \n```\npackage com.choxsu.elastic.config;\n\n/**\n * @author choxsu\n */\n\nimport java.lang.annotation.*;\n\n/**\n * Jfinal事物交给spring管理注解\n * 目前只支持用在方法上\n */\n@Inherited\n@Target({ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface JFinalTx {\n\n}\n```\n### 使用\n#### TestController \n```\npackage com.choxsu.elastic.controller;\n\nimport com.choxsu.elastic.service.TestService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n/**\n * @author choxsu\n */\n@RestController\n@RequestMapping(value = {\"/test/v1\"})\npublic class TestController {\n\n\n    @Autowired\n    private TestService testService;\n\n    @GetMapping(value = \"/testTran\")\n    public Object testTran(){\n\n\n        return testService.testTran();\n    }\n}\n\n```\n#### TestService\n```\npackage com.choxsu.elastic.service;\n\nimport com.choxsu.elastic.config.JFinalTx;\nimport com.jfinal.kit.Ret;\nimport com.jfinal.plugin.activerecord.Db;\nimport com.jfinal.plugin.activerecord.Record;\nimport org.springframework.stereotype.Service;\n\n\n/**\n * @author choxsu\n */\n@Service\npublic class TestService {\n\n    /**\n     * 事物测试\n     *\n     * @return\n     */\n    @JFinalTx\n    public Object testTran() {\n        Record record = new Record();\n        record.set(\"id\", 10);\n        Db.save(\"test\", record);\n        if (true) {\n            throw new RuntimeException(\"test\");\n        }\n        return Ret.by(\"msg\", \"success\");\n    }\n}\n\n```\n##### sql执行了\n```\nSql: insert into `test`(`id`) values(?)\n```\n##### 但是数据库没有数据\n![image.png](https://upload-images.jianshu.io/upload_images/7463793-cb026e4ac9652bef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n到此证明事物拦截成功，可以使用spring来管理ActiveRecordPlugin的事物了\n\n#### 去掉`throw new RuntimeException(\"test\");`的效果\n##### sql\n```\nSql: insert into `test`(`id`) values(?)\n```\n##### 数据库结果\n![image.png](https://upload-images.jianshu.io/upload_images/7463793-afcb50b9767dbadc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n ', '2018-06-14 12:09:34', '2018-06-13 17:28:29', 143, 0, 0, 'blog', 0, 1, NULL);
INSERT INTO `blog` VALUES (16, 1, '在linux(CentOS7.2 64bit)上安装elasticSearch 6.0.0', '<h3 id=\"h3-01-\"><a name=\"01 准备工作及下载\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>01 准备工作及下载</h3><h5 id=\"h5-1-es-es-root-\"><a name=\"1)创建一个es专门的用户（必须），因为es不能用root用户启动\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1)创建一个es专门的用户（必须），因为es不能用root用户启动</h5><pre><code>useradd es -m\npasswd &lt;input es&gt;\nmkdir -p /usr/local/es/\nchown -R es /usr/local/es/\n</code></pre><h5 id=\"h5-2-es-\"><a name=\"2)切换到es用户下，下载安装包\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>2)切换到es用户下，下载安装包</h5><pre><code>su es\ncd /usr/local/es\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zip\nunzip elasticsearch-6.0.0.zip\ncd elasticsearch-6.0.0/\nls\nbin  config  data  lib  LICENSE.txt  logs  modules  nohup.out  NOTICE.txt  plugins  README.textile\n</code></pre><h5 id=\"h5-3-\"><a name=\"3)修改配置文件\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>3)修改配置文件</h5><pre><code>cd config/\nvim elasticsearch.yml\n</code></pre><pre><code>cluster.name: choxsu-es\n#path.data: /path/to/data  # 默认即可\n#path.logs: /path/to/logs  # 默认即可\nnetwork.host: 0.0.0.0\nhttp.port: 9200\n</code></pre><h5 id=\"h5-4-jvm-\"><a name=\"4)修改jvm内存大小\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>4)修改jvm内存大小</h5><pre><code>vim config/jvm.options\n</code></pre><pre><code>-Xms256m\n-Xmx256m\n</code></pre><h3 id=\"h3-02-\"><a name=\"02 解决启动时报错\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>02 解决启动时报错</h3><h5 id=\"h5-1-\"><a name=\"1) 启动命令\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1) 启动命令</h5><p>这里注意啦，这里是后台启动，要发现错误的话，去logs目录下查看。</p>\n<pre><code>nohup ./bin/elasticsearch &amp;\n</code></pre><h5 id=\"h5-2-\"><a name=\"2)查看错误信息\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>2)查看错误信息</h5><p>注意：ElasticSearch6.0.0必须jdk1.8以上支持；所以请记得在<em>/etc/profile</em>文件中配置jdk环境变量</p>\n<pre><code>export JAVA_HOME=/opt/java/jdk1.8.0_161\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nexport PATH=$JAVA_HOME/bin:$PATH\n</code></pre><p>直接查看nohup输出信息</p>\n<pre><code>tail -f nohup.out\n</code></pre><p>或者查看日志输出查看错误信息</p>\n<pre><code>tail -f logs/choxsu-es.log\n</code></pre><p>核心错误信息</p>\n<pre><code>[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n[4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk\n</code></pre><h5 id=\"h5-3-\"><a name=\"3)解决办法\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>3)解决办法</h5><pre><code>1）max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]\n原因：无法创建本地文件问题,用户最大可创建文件数太小\n解决方案：切换到root用户，编辑limits.conf配置文件， 添加类似如下内容：\nvi /etc/security/limits.conf\n添加如下内容:\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n\n备注：* 代表Linux所有用户名称（比如 hadoop）\n需要保存、退出、重新登录才可生效。\n\n2）max number of threads [1024] for user [es] likely too low, increase to at least [4096]\n原因：无法创建本地线程问题,用户最大可创建线程数太小\n解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件。\nvi /etc/security/limits.d/90-nproc.conf\n找到如下内容：\n* soft nproc 1024\n#修改为\n* soft nproc 4096\n\n3）max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]\n原因：最大虚拟内存太小\nroot用户执行命令：\n[root@localhost ~]# sysctl -w vm.max_map_count=262144\n\n4）system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk\n原因：Centos6不支持SecComp，而ES5.4.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。\n详见 ：https://github.com/elastic/elasticsearch/issues/22899\n\n解决方法：在elasticsearch.yml中新增配置bootstrap.system_call_filter，设为false，注意要在Memory下面:\nbootstrap.memory_lock: false\nbootstrap.system_call_filter: false\n\n以上问题解决后，es启动成功了，但又遇到了新的问题，本地机器无法访问虚拟机的服务，两个原因：\n1）9200被限制为本机访问，需要在es的配置文件elasticsearch.yml中新增配置：\n    network.bind_host:0.0.0.0\n2）关闭虚拟机防火墙\n\n解决了这个两个问题后，本地能够顺利访问虚拟机的ES服务了。\n</code></pre><p>注意，以上虚拟内存的更改，每次重启系统之后都要重新设置</p>\n<pre><code>ysctl -w vm.max_map_count=262144\n</code></pre><h5 id=\"h5-4-\"><a name=\"4)解决完了之后，再次启动服务（先杀后启）\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>4)解决完了之后，再次启动服务（先杀后启）</h5><pre><code>ps -ef|grep elasticsearch|grep bootstrap |awk &#39;{print $2}&#39; |xargs kill -9\nnohup ./bin/elasticsearch &amp;\n</code></pre><h3 id=\"h3-03-es\"><a name=\"03 访问es\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>03 访问es</h3><h5 id=\"h5-1-curl-\"><a name=\"1)使用curl测试，因为使用的是云服务器部署\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1)使用curl测试，因为使用的是云服务器部署</h5><p>curl <a href=\"http://localhost:9200/?pretty\">http://localhost:9200/?pretty</a><br>得到的内容</p>\n<pre><code>{\n  &quot;name&quot; : &quot;Nd1M7rH&quot;,\n  &quot;cluster_name&quot; : &quot;choxsu-es&quot;,\n  &quot;cluster_uuid&quot; : &quot;593zK7LuTZOvfcdgt09ZXQ&quot;,\n  &quot;version&quot; : {\n    &quot;number&quot; : &quot;6.0.0&quot;,\n    &quot;build_hash&quot; : &quot;8f0685b&quot;,\n    &quot;build_date&quot; : &quot;2017-11-10T18:41:22.859Z&quot;,\n    &quot;build_snapshot&quot; : false,\n    &quot;lucene_version&quot; : &quot;7.0.1&quot;,\n    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,\n    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;\n  },\n  &quot;tagline&quot; : &quot;You Know, for Search&quot;\n}\n</code></pre><h3 id=\"h3-04-\"><a name=\"04 使用原生方式创建索引\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>04 使用原生方式创建索引</h3><p>使用 Xput创建索引</p>\n<pre><code>curl -XPUT &#39;http://localhost:9200/twitter/doc/1?pretty&#39; -H &#39;Content-Type: application/json&#39; -d &#39;\n{\n    &quot;user&quot;: &quot;kimchy&quot;,\n    &quot;post_date&quot;: &quot;2009-11-15T13:12:00&quot;,\n    &quot;message&quot;: &quot;Trying out Elasticsearch, so far so good?&quot;\n}&#39;\n\ncurl -XPUT &#39;http://localhost:9200/twitter/doc/2?pretty&#39; -H &#39;Content-Type: application/json&#39; -d &#39;\n{\n    &quot;user&quot;: &quot;kimchy&quot;,\n    &quot;post_date&quot;: &quot;2009-11-15T13:12:00&quot;,\n    &quot;message&quot;: &quot;Trying out Elasticsearch, so far so good?&quot;\n}&#39;\n\ncurl -XPUT &#39;http://localhost:9200/twitter/doc/3?pretty&#39; -H &#39;Content-Type: application/json&#39; -d &#39;\n{\n    &quot;user&quot;: &quot;kimchy&quot;,\n    &quot;post_date&quot;: &quot;2009-11-15T13:12:00&quot;,\n    &quot;message&quot;: &quot;Trying out Elasticsearch, so far so good?&quot;\n}&#39;\n</code></pre><p>查询数据</p>\n<pre><code>curl -XGET &#39;http://localhost:9200/twitter/doc/1?pretty=true&#39;\ncurl -XGET &#39;http://localhost:9200/twitter/doc/2?pretty=true&#39;\ncurl -XGET &#39;http://localhost:9200/twitter/doc/3?pretty=true&#39;\n</code></pre><p>搜索数据<br>通过字进行查询：q=user:kimchy</p>\n<pre><code>curl -XGET &#39;http://localhost:9200/twitter/_search?q=user:kimchy&amp;pretty=true&#39;\n</code></pre><pre><code>curl -XGET &#39;http://localhost:9200/twitter/_search?q=user:kimchy&amp;pretty=true&#39;\n</code></pre><p>通过JSON的方式进行查询</p>\n<pre><code>curl -XGET &#39;http://localhost:9200/twitter/_search?pretty=true&#39; -H &#39;Content-Type: application/json&#39; -d &#39;\n{\n    &quot;query&quot; : {\n        &quot;match_all&quot; : {}\n    }\n}&#39;\n</code></pre><p>通过JSON的方式查询，查询的时候指定区间</p>\n<pre><code>curl -XGET &#39;http://localhost:9200/twitter/_search?pretty=true&#39; -H &#39;Content-Type: application/json&#39; -d &#39;\n{\n    &quot;query&quot; : {\n        &quot;range&quot; : {\n            &quot;post_date&quot; : { &quot;from&quot; : &quot;2009-11-15T13:00:00&quot;, &quot;to&quot; : &quot;2009-11-15T14:00:00&quot; }\n        }\n    }\n}&#39;\n</code></pre><p>那教程就到这里 </p>\n', ' ###01 准备工作及下载\n#####1)创建一个es专门的用户（必须），因为es不能用root用户启动\n```\nuseradd es -m\npasswd <input es>\nmkdir -p /usr/local/es/\nchown -R es /usr/local/es/\n```\n#####2)切换到es用户下，下载安装包\n```\nsu es\ncd /usr/local/es\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zip\nunzip elasticsearch-6.0.0.zip\ncd elasticsearch-6.0.0/\nls\nbin  config  data  lib  LICENSE.txt  logs  modules  nohup.out  NOTICE.txt  plugins  README.textile\n```\n#####3)修改配置文件\n```\ncd config/\nvim elasticsearch.yml\n```\n```\ncluster.name: choxsu-es\n#path.data: /path/to/data  # 默认即可\n#path.logs: /path/to/logs  # 默认即可\nnetwork.host: 0.0.0.0\nhttp.port: 9200  \n```\n#####4)修改jvm内存大小\n```\nvim config/jvm.options\n```\n```\n-Xms256m\n-Xmx256m\n```\n###02 解决启动时报错\n\n #####1) 启动命令\n这里注意啦，这里是后台启动，要发现错误的话，去logs目录下查看。\n```\nnohup ./bin/elasticsearch &\n```\n#####2)查看错误信息\n注意：ElasticSearch6.0.0必须jdk1.8以上支持；所以请记得在*/etc/profile*文件中配置jdk环境变量\n```\nexport JAVA_HOME=/opt/java/jdk1.8.0_161\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n直接查看nohup输出信息\n```\ntail -f nohup.out\n```\n或者查看日志输出查看错误信息\n```\ntail -f logs/choxsu-es.log\n```\n核心错误信息\n```\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n[4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk\n```\n#####3)解决办法\n```\n1）max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]\n原因：无法创建本地文件问题,用户最大可创建文件数太小\n解决方案：切换到root用户，编辑limits.conf配置文件， 添加类似如下内容：\nvi /etc/security/limits.conf\n添加如下内容:\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n\n备注：* 代表Linux所有用户名称（比如 hadoop）\n需要保存、退出、重新登录才可生效。\n\n2）max number of threads [1024] for user [es] likely too low, increase to at least [4096]\n原因：无法创建本地线程问题,用户最大可创建线程数太小\n解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件。\nvi /etc/security/limits.d/90-nproc.conf\n找到如下内容：\n* soft nproc 1024\n#修改为\n* soft nproc 4096\n\n3）max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]\n原因：最大虚拟内存太小\nroot用户执行命令：\n[root@localhost ~]# sysctl -w vm.max_map_count=262144\n\n4）system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk\n原因：Centos6不支持SecComp，而ES5.4.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。\n详见 ：https://github.com/elastic/elasticsearch/issues/22899\n\n解决方法：在elasticsearch.yml中新增配置bootstrap.system_call_filter，设为false，注意要在Memory下面:\nbootstrap.memory_lock: false\nbootstrap.system_call_filter: false\n\n以上问题解决后，es启动成功了，但又遇到了新的问题，本地机器无法访问虚拟机的服务，两个原因：\n1）9200被限制为本机访问，需要在es的配置文件elasticsearch.yml中新增配置：\n    network.bind_host:0.0.0.0\n2）关闭虚拟机防火墙\n\n解决了这个两个问题后，本地能够顺利访问虚拟机的ES服务了。\n```\n注意，以上虚拟内存的更改，每次重启系统之后都要重新设置\n```\nysctl -w vm.max_map_count=262144\n```\n#####4)解决完了之后，再次启动服务（先杀后启）\n```\nps -ef|grep elasticsearch|grep bootstrap |awk \'{print $2}\' |xargs kill -9\nnohup ./bin/elasticsearch &\n```\n###03 访问es\n#####1)使用curl测试，因为使用的是云服务器部署\ncurl http://localhost:9200/?pretty\n得到的内容\n```\n{\n  \"name\" : \"Nd1M7rH\",\n  \"cluster_name\" : \"choxsu-es\",\n  \"cluster_uuid\" : \"593zK7LuTZOvfcdgt09ZXQ\",\n  \"version\" : {\n    \"number\" : \"6.0.0\",\n    \"build_hash\" : \"8f0685b\",\n    \"build_date\" : \"2017-11-10T18:41:22.859Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.0.1\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\n###04 使用原生方式创建索引\n使用 Xput创建索引\n\n```\ncurl -XPUT \'http://localhost:9200/twitter/doc/1?pretty\' -H \'Content-Type: application/json\' -d \'\n{\n    \"user\": \"kimchy\",\n    \"post_date\": \"2009-11-15T13:12:00\",\n    \"message\": \"Trying out Elasticsearch, so far so good?\"\n}\'\n\ncurl -XPUT \'http://localhost:9200/twitter/doc/2?pretty\' -H \'Content-Type: application/json\' -d \'\n{\n    \"user\": \"kimchy\",\n    \"post_date\": \"2009-11-15T13:12:00\",\n    \"message\": \"Trying out Elasticsearch, so far so good?\"\n}\'\n\ncurl -XPUT \'http://localhost:9200/twitter/doc/3?pretty\' -H \'Content-Type: application/json\' -d \'\n{\n    \"user\": \"kimchy\",\n    \"post_date\": \"2009-11-15T13:12:00\",\n    \"message\": \"Trying out Elasticsearch, so far so good?\"\n}\'\n```\n查询数据\n```\ncurl -XGET \'http://localhost:9200/twitter/doc/1?pretty=true\'\ncurl -XGET \'http://localhost:9200/twitter/doc/2?pretty=true\'\ncurl -XGET \'http://localhost:9200/twitter/doc/3?pretty=true\'\n```\n搜索数据\n通过字进行查询：q=user:kimchy\n```\ncurl -XGET \'http://localhost:9200/twitter/_search?q=user:kimchy&pretty=true\'\n```\n```\ncurl -XGET \'http://localhost:9200/twitter/_search?q=user:kimchy&pretty=true\'\n```\n通过JSON的方式进行查询\n```\ncurl -XGET \'http://localhost:9200/twitter/_search?pretty=true\' -H \'Content-Type: application/json\' -d \'\n{\n    \"query\" : {\n        \"match_all\" : {}\n    }\n}\'\n```\n通过JSON的方式查询，查询的时候指定区间\n```\ncurl -XGET \'http://localhost:9200/twitter/_search?pretty=true\' -H \'Content-Type: application/json\' -d \'\n{\n    \"query\" : {\n        \"range\" : {\n            \"post_date\" : { \"from\" : \"2009-11-15T13:00:00\", \"to\" : \"2009-11-15T14:00:00\" }\n        }\n    }\n}\'\n```\n那教程就到这里 \n', '2018-06-14 17:02:23', '2018-06-14 17:02:23', 23, 0, 0, 'blog', 0, 16, NULL);
INSERT INTO `blog` VALUES (24, 1, 'mysql数据同步elasticsearch（es）全文检索容器', '<h3 id=\"h3--elasticsearch-es-6-0-0-windows10-\"><a name=\"一、安装ElasticSearch(下面统称es,版本6.0.0,环境windows10)\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>一、安装ElasticSearch(下面统称es,版本6.0.0,环境windows10)</h3><p>直接上下载地址：<a href=\"https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zip\">https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zip</a></p>\n<h6 id=\"h6--\"><a name=\"解压后目录如下：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>解压后目录如下：</h6><p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-ef3f1fffde2c8dcb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"1\"></p>\n<p>启动es，<code>./bin/elasticsearch.bat</code>；启动成功如图</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-676087ab660fca8e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"2\"></p>\n<p>默认cluster_name是elasticsearch和端口9200可以修改，需要修改在config/elasticsearch.yml;上图</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-467b09415179d627?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"3\"></p>\n<h3 id=\"h3--logstash\"><a name=\"二、安装logstash\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>二、安装logstash</h3><p>下载地址：<a href=\"https://artifacts.elastic.co/downloads/logstash/logstash-6.0.0.zip\">https://artifacts.elastic.co/downloads/logstash/logstash-6.0.0.zip</a></p>\n<h6 id=\"h6-u89E3u538Bu76EEu5F55\"><a name=\"解压目录\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>解压目录</h6><p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-776606a94e098cb3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"4\"></p>\n<p>先安装logstash-input-jdbc插件<br><code>./bin/logstash-plugin.bat install logstash-input-jdbc</code></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-1160c85b82c39f7f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"5\"></p>\n<p>在logstash目录下创建config-mysql，见图4</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-1fca588e689bba1b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"6\"></p>\n<p>创建配置文件load_data.conf，配置文件随便取名，可以创建sql文件，也可以在conf配置文件中定义，具体下面有说明</p>\n<h6 id=\"h6-u5148u4E0Au914Du7F6Eu6587u4EF6u5185u5BB9\"><a name=\"先上配置文件内容\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>先上配置文件内容</h6><pre><code>input {\n    stdin {\n    }\n    jdbc {\n      jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/jfinal_club?characterEncoding=utf8&amp;useSSL=false&quot;\n      jdbc_user =&gt; &quot;root&quot;\n      jdbc_password =&gt; &quot;root&quot;\n      jdbc_driver_library =&gt; &quot;D:/ELK/6.0.0/logstash-6.0.0/config-mysql/mysql-connector-java-5.1.43.jar&quot;\n      jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot;\n      jdbc_paging_enabled =&gt; &quot;true&quot;\n      jdbc_page_size =&gt; &quot;50000&quot;\n      statement_filepath =&gt; &quot;D:/ELK/6.0.0/logstash-6.0.0/config-mysql/store_list.sql&quot;\n      schedule =&gt; &quot;* * * * *&quot;\n      use_column_value =&gt; false\n      record_last_run =&gt; true\n      last_run_metadata_path =&gt; &quot;D:/ELK/6.0.0/logstash-6.0.0/config-mysql/run/store_list&quot;\n      type =&gt; &quot;sl&quot;\n    }\n\njdbc {\n      jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/jfinal_club?characterEncoding=utf8&amp;useSSL=false&quot;\n      jdbc_user =&gt; &quot;root&quot;\n      jdbc_password =&gt; &quot;root&quot;\n      jdbc_driver_library =&gt; &quot;D:/ELK/6.0.0/logstash-6.0.0/config-mysql/mysql-connector-java-5.1.43.jar&quot;\n      jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot;\n      jdbc_paging_enabled =&gt; &quot;true&quot;\n      jdbc_page_size =&gt; &quot;50000&quot;\n      statement =&gt; &quot;select * from store where updated &gt; date_add(:sql_last_value, interval 8 hour)&quot;\n      schedule =&gt; &quot;* * * * *&quot;\n      use_column_value =&gt; false\n      record_last_run =&gt; true\n      last_run_metadata_path =&gt; &quot;D:/ELK/6.0.0/logstash-6.0.0/config-mysql/run/store_s&quot;\n      type =&gt; &quot;st&quot;\n    }\n}\n\nfilter {\n    json {\n        source =&gt; &quot;message&quot;\n        remove_field =&gt; [&quot;message&quot;]\n    }\n}\n\noutput {\n\n    if[type] == &quot;sl&quot;{\n        elasticsearch {\n              hosts =&gt; [&quot;127.0.0.1:9200&quot;]\n              index =&gt; &quot;store_list&quot;\n              document_type =&gt; &quot;jdbc&quot;\n              document_id =&gt; &quot;%{store_id}}&quot;\n        }\n    }\n\n    if[type] == &quot;st&quot;{\n        elasticsearch {\n              hosts =&gt; [&quot;127.0.0.1:9200&quot;]\n              index =&gt; &quot;store_st&quot;\n              document_type =&gt; &quot;jdbc&quot;\n              document_id =&gt; &quot;%{id}}&quot;\n        }\n    }\n\n    stdout {\n        codec =&gt; json_lines\n    }\n}\n</code></pre><p>字段解释;具体的见：<a href=\"https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html\">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html</a></p>\n<h6 id=\"h6--6-run-sql_last_value-\"><a name=\"图6中有个run目录，在这里是用来存放:sql_last_value的时间值的\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>图6中有个run目录，在这里是用来存放:sql_last_value的时间值的</h6><h5 id=\"h5-store_list-sql\"><a name=\"store_list.sql\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>store_list.sql</h5><p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-582c47289732bf76?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"7\"></p>\n<h5 id=\"h5--es-index\"><a name=\"先在es中生成index\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>先在es中生成index</h5><pre><code>PUT /store_list\n{\n    &quot;settings&quot;: {\n    &quot;number_of_shards&quot;: 3,\n    &quot;number_of_replicas&quot;: 1\n  },\n  &quot;mappings&quot;: {\n    &quot;jdbc&quot;: {\n      &quot;properties&quot;: {\n        &quot;@timestamp&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;@version&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;store_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;store_name&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;uid&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;telephone&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;street_id&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;detail&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;address&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;store_created&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;store_updated&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;detail_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;type_name&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;tag&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;overall_rating&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;navi_location_lng&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;navi_location_lat&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;detail_url&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;comment_num&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;detail_created&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;detail_updated&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;location_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;lng&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;lat&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        }\n      }\n    }\n  }\n}\n</code></pre><p>上面这种方式可以通过es管理工具执行，比如<code>kibana-&gt;dev tools</code>;或者使用<code>curl</code>的方式也可以</p>\n<pre><code>curl -XPUT &quot;http://localhost:9200/store_list&quot; -H &#39;Content-Type: application/json&#39; -d&#39;\n{\n  &quot;settings&quot;: {\n    &quot;number_of_shards&quot;: 3,\n    &quot;number_of_replicas&quot;: 1\n  },\n  &quot;mappings&quot;: {\n    &quot;jdbc&quot;: {\n      &quot;properties&quot;: {\n        &quot;@timestamp&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;@version&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;store_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;store_name&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;uid&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;telephone&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;street_id&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;detail&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;address&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;store_created&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;store_updated&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;detail_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;type_name&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;tag&quot;: {\n          &quot;type&quot;: &quot;keyword&quot;\n        },\n        &quot;overall_rating&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;navi_location_lng&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;navi_location_lat&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;detail_url&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;comment_num&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;detail_created&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;detail_updated&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;location_id&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        },\n        &quot;lng&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        },\n        &quot;lat&quot;: {\n          &quot;type&quot;: &quot;double&quot;\n        }\n      }\n    }\n  }\n}&#39;\n</code></pre><p>然后通过<a href=\"http://localhost:9200/store_list/查看字段生成情况\">http://localhost:9200/store_list/查看字段生成情况</a></p>\n<p>store_list就是index,相当于数据库的database</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-64cbb0722cbdb9c3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"8\"></p>\n<p>然后回到logstash目录下</p>\n<p>执行 <code>nohup.exe ./bin/logstash.bat -f config-mysql/load_data.conf &amp;</code></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-9937af7164c47358?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"9\"></p>\n<p>最好加上&amp; 结尾，后台运行</p>\n<p>然后看数据库同步情况</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-b40da0c9563f517a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"10\"></p>\n<p>可能有些细节没能写全，如果在集成中遇到什么情况，可以评论指出</p>\n', ' ### 一、安装ElasticSearch(下面统称es,版本6.0.0,环境windows10)\n\n直接上下载地址：https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zip\n###### 解压后目录如下：\n\n![1](http://upload-images.jianshu.io/upload_images/7463793-ef3f1fffde2c8dcb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n启动es，``./bin/elasticsearch.bat ``；启动成功如图\n\n![2](http://upload-images.jianshu.io/upload_images/7463793-676087ab660fca8e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n默认cluster_name是elasticsearch和端口9200可以修改，需要修改在config/elasticsearch.yml;上图\n\n![3](http://upload-images.jianshu.io/upload_images/7463793-467b09415179d627?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### 二、安装logstash\n\n下载地址：https://artifacts.elastic.co/downloads/logstash/logstash-6.0.0.zip\n\n######解压目录\n\n![4](http://upload-images.jianshu.io/upload_images/7463793-776606a94e098cb3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n先安装logstash-input-jdbc插件 \n``./bin/logstash-plugin.bat install logstash-input-jdbc``\n\n![5](http://upload-images.jianshu.io/upload_images/7463793-1160c85b82c39f7f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n在logstash目录下创建config-mysql，见图4\n\n![6](http://upload-images.jianshu.io/upload_images/7463793-1fca588e689bba1b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n创建配置文件load_data.conf，配置文件随便取名，可以创建sql文件，也可以在conf配置文件中定义，具体下面有说明\n\n######先上配置文件内容\n```\ninput {\n    stdin {\n    }\n    jdbc {\n      jdbc_connection_string => \"jdbc:mysql://127.0.0.1:3306/jfinal_club?characterEncoding=utf8&useSSL=false\"\n      jdbc_user => \"root\"\n      jdbc_password => \"root\"\n      jdbc_driver_library => \"D:/ELK/6.0.0/logstash-6.0.0/config-mysql/mysql-connector-java-5.1.43.jar\"\n      jdbc_driver_class => \"com.mysql.jdbc.Driver\"\n      jdbc_paging_enabled => \"true\"\n      jdbc_page_size => \"50000\"\n      statement_filepath => \"D:/ELK/6.0.0/logstash-6.0.0/config-mysql/store_list.sql\"\n      schedule => \"* * * * *\"\n      use_column_value => false\n      record_last_run => true\n      last_run_metadata_path => \"D:/ELK/6.0.0/logstash-6.0.0/config-mysql/run/store_list\"\n      type => \"sl\"\n    }\n\njdbc {\n      jdbc_connection_string => \"jdbc:mysql://127.0.0.1:3306/jfinal_club?characterEncoding=utf8&useSSL=false\"\n      jdbc_user => \"root\"\n      jdbc_password => \"root\"\n      jdbc_driver_library => \"D:/ELK/6.0.0/logstash-6.0.0/config-mysql/mysql-connector-java-5.1.43.jar\"\n      jdbc_driver_class => \"com.mysql.jdbc.Driver\"\n      jdbc_paging_enabled => \"true\"\n      jdbc_page_size => \"50000\"\n      statement => \"select * from store where updated > date_add(:sql_last_value, interval 8 hour)\"\n      schedule => \"* * * * *\"\n      use_column_value => false\n      record_last_run => true\n      last_run_metadata_path => \"D:/ELK/6.0.0/logstash-6.0.0/config-mysql/run/store_s\"\n      type => \"st\"\n    }\n}\n\nfilter {\n    json {\n        source => \"message\"\n        remove_field => [\"message\"]\n    }\n}\n\noutput {\n\n    if[type] == \"sl\"{\n        elasticsearch {\n              hosts => [\"127.0.0.1:9200\"]\n              index => \"store_list\"\n              document_type => \"jdbc\"\n              document_id => \"%{store_id}}\"\n        }\n    }\n\n    if[type] == \"st\"{\n        elasticsearch {\n              hosts => [\"127.0.0.1:9200\"]\n              index => \"store_st\"\n              document_type => \"jdbc\"\n              document_id => \"%{id}}\"\n        }\n    }\n\n    stdout {\n        codec => json_lines\n    }\n}\n```\n字段解释;具体的见：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html\n\n\n###### 图6中有个run目录，在这里是用来存放:sql_last_value的时间值的\n\n##### store_list.sql\n\n![7](http://upload-images.jianshu.io/upload_images/7463793-582c47289732bf76?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n##### 先在es中生成index\n```\nPUT /store_list\n{\n    \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"jdbc\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"@version\": {\n          \"type\": \"keyword\"\n        },\n        \"store_id\": {\n          \"type\": \"long\"\n        },\n        \"store_name\": {\n          \"type\": \"keyword\"\n        },\n        \"uid\": {\n          \"type\": \"text\"\n        },\n        \"telephone\": {\n          \"type\": \"text\"\n        },\n        \"street_id\": {\n          \"type\": \"text\"\n        },\n        \"detail\": {\n          \"type\": \"keyword\"\n        },\n        \"address\": {\n          \"type\": \"keyword\"\n        },\n        \"store_created\": {\n          \"type\": \"date\"\n        },\n        \"store_updated\": {\n          \"type\": \"date\"\n        },\n        \"detail_id\": {\n          \"type\": \"long\"\n        },\n        \"type_name\": {\n          \"type\": \"text\"\n        },\n        \"tag\": {\n          \"type\": \"keyword\"\n        },\n        \"overall_rating\": {\n          \"type\": \"text\"\n        },\n        \"navi_location_lng\": {\n          \"type\": \"double\"\n        },\n        \"navi_location_lat\": {\n          \"type\": \"double\"\n        },\n        \"detail_url\": {\n          \"type\": \"text\"\n        },\n        \"comment_num\": {\n          \"type\": \"integer\"\n        },\n        \"detail_created\": {\n          \"type\": \"date\"\n        },\n        \"detail_updated\": {\n          \"type\": \"date\"\n        },\n        \"location_id\": {\n          \"type\": \"long\"\n        },\n        \"lng\": {\n          \"type\": \"double\"\n        },\n        \"lat\": {\n          \"type\": \"double\"\n        }\n      }\n    }\n  }\n}\n```\n上面这种方式可以通过es管理工具执行，比如`kibana->dev tools`;或者使用`curl`的方式也可以\n```\ncurl -XPUT \"http://localhost:9200/store_list\" -H \'Content-Type: application/json\' -d\'\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"jdbc\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"@version\": {\n          \"type\": \"keyword\"\n        },\n        \"store_id\": {\n          \"type\": \"long\"\n        },\n        \"store_name\": {\n          \"type\": \"keyword\"\n        },\n        \"uid\": {\n          \"type\": \"text\"\n        },\n        \"telephone\": {\n          \"type\": \"text\"\n        },\n        \"street_id\": {\n          \"type\": \"text\"\n        },\n        \"detail\": {\n          \"type\": \"keyword\"\n        },\n        \"address\": {\n          \"type\": \"keyword\"\n        },\n        \"store_created\": {\n          \"type\": \"date\"\n        },\n        \"store_updated\": {\n          \"type\": \"date\"\n        },\n        \"detail_id\": {\n          \"type\": \"long\"\n        },\n        \"type_name\": {\n          \"type\": \"text\"\n        },\n        \"tag\": {\n          \"type\": \"keyword\"\n        },\n        \"overall_rating\": {\n          \"type\": \"text\"\n        },\n        \"navi_location_lng\": {\n          \"type\": \"double\"\n        },\n        \"navi_location_lat\": {\n          \"type\": \"double\"\n        },\n        \"detail_url\": {\n          \"type\": \"text\"\n        },\n        \"comment_num\": {\n          \"type\": \"integer\"\n        },\n        \"detail_created\": {\n          \"type\": \"date\"\n        },\n        \"detail_updated\": {\n          \"type\": \"date\"\n        },\n        \"location_id\": {\n          \"type\": \"long\"\n        },\n        \"lng\": {\n          \"type\": \"double\"\n        },\n        \"lat\": {\n          \"type\": \"double\"\n        }\n      }\n    }\n  }\n}\'\n```\n然后通过http://localhost:9200/store_list/查看字段生成情况\n\nstore_list就是index,相当于数据库的database\n\n![8](http://upload-images.jianshu.io/upload_images/7463793-64cbb0722cbdb9c3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n然后回到logstash目录下\n\n执行 ``nohup.exe ./bin/logstash.bat -f config-mysql/load_data.conf &``\n\n![9](http://upload-images.jianshu.io/upload_images/7463793-9937af7164c47358?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n最好加上& 结尾，后台运行\n\n然后看数据库同步情况\n\n![10](http://upload-images.jianshu.io/upload_images/7463793-b40da0c9563f517a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n可能有些细节没能写全，如果在集成中遇到什么情况，可以评论指出\n', '2018-06-14 17:04:36', '2018-06-14 17:04:36', 25, 0, 0, 'blog', 0, 1, NULL);
INSERT INTO `blog` VALUES (25, 1, 'SpringBoot中使用JFinal的ActiveRecordPlugin插件', '<h4 id=\"h4--jfinal-3-3-5-13-activerecordactiverecordplugin-java-web-java-web-start-\"><a name=\"在写分享前先看了看jfinal-3.3的文档章节：5.13 任意环境下使用 ActiveRecordActiveRecordPlugin 可以独立于 java web 环境运行在任何普通的 java 程序中，使用方式极度简单，相对于 web 项目只需要手动调用一下其 start() 方法即可立即使用。\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>在写分享前先看了看jfinal-3.3的文档章节：5.13 任意环境下使用 ActiveRecordActiveRecordPlugin 可以独立于 java web 环境运行在任何普通的 java 程序中，使用方式极度简单，相对于 web 项目只需要手动调用一下其 start() 方法即可立即使用。</h4><h5 id=\"h5--\"><a name=\"以下是代码示例：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>以下是代码示例：</h5><pre><code>public class ActiveRecordTest{\n   public static void main(String[] args){\n       DruidPlugin dp=new DruidPlugin(&quot;localhost&quot;,&quot;userName&quot;,&quot;password&quot;);\n       ActiveRecordPlugin arp=new ActiveRecordPlugin(dp);\n       arp.addMapping(&quot;blog&quot;,Blog.class);\n       // 与web环境唯一的不同是要手动调用一次相关插件的start()方法\n       dp.start();\n       arp.start();\n       // 通过上面简单的几行代码，即可立即开始使用\n      newBlog().set(&quot;title&quot;,&quot;title&quot;).set(&quot;content&quot;,&quot;cxt text&quot;).save();\n       Blog.dao.findById(123);\n   }\n}\n</code></pre><p>注意：<code>ActiveRecordPlugin</code> 所依赖的其它插件也必须手动调用一下 start()方法，如上例中的<code>dp.start()</code></p>\n<h5 id=\"h5--\"><a name=\"下面进入正题：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>下面进入正题：</h5><p>创建一个插件类</p>\n<h6 id=\"h6-activerecordpluginconfig-\"><a name=\"ActiveRecordPluginConfig类\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>ActiveRecordPluginConfig类</h6><pre><code>package com.choxsu.elastic.config;\nimport com.alibaba.druid.filter.stat.StatFilter;\nimport com.alibaba.druid.wall.WallFilter;\nimport com.choxsu.elastic.entity._MappingKit;\nimport com.jfinal.plugin.activerecord.ActiveRecordPlugin;\nimport com.jfinal.plugin.druid.DruidPlugin;\nimport com.jfinal.template.source.ClassPathSourceFactory;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport java.sql.Connection;\n/**\n * @author chox su\n * @date 2017/11/29 10:16\n */\n @Configuration\n public class ActiveRecordPluginConfig{\n    @Value(&quot;${spring.datasource.username}&quot;)\n    private String username;\n\n    @Value(&quot;${spring.datasource.password}&quot;)\n    private String password;\n\n    @Value(&quot;${spring.datasource.url}&quot;)\n    private String url;\n\n    @Bean\n    public ActiveRecordPlug ininitActiveRecordPlugin(){\n        DruidPlugindruidPlugin=newDruidPlugin(url,username,password);\n        // 加强数据库安全\n        WallFilterwallFilter=newWallFilter();\n        wallFilter.setDbType(&quot;mysql&quot;);\n        druidPlugin.addFilter(wallFilter);\n        // 添加 StatFilter 才会有统计数据\n        // druidPlugin.addFilter(new StatFilter());\n        // 必须手动调用start\n        druidPlugin.start();\n        ActiveRecordPlugin arp=new ActiveRecordPlugin(druidPlugin);\n        arp.setTransactionLevel(Connection.TRANSACTION_READ_COMMITTED);\n        _MappingKit.mapping(arp);\n        arp.setShowSql(false);\n        arp.getEngine().setSourceFactory(new ClassPathSourceFactory());\n        arp.addSqlTemplate(&quot;/sql/all_sqls.sql&quot;);\n        // 必须手动调用start\n        arp.start();\n        return arp;\n    }\n}\n</code></pre><h6 id=\"h6-application-yml-\"><a name=\"application.yml配置文件\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>application.yml配置文件</h6><pre><code>server:\n  port: 1013\nspring:\n  application:\n    name: name:elastic\n  datasource:\n    driver-class-name: com.mysql.jdbc.Driver\n    username: root\n    password: root\n    url: jdbc:mysql://192.168.3.44:3306/jfinal_club?characterEncoding=utf8&amp;useSSL=false\n</code></pre><h6 id=\"h6-pom-xml-\"><a name=\"pom.xml配置文件\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>pom.xml配置文件</h6><pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.zgxl.market&lt;/groupId&gt;\n  &lt;artifactId&gt;fastjson&lt;/artifactId&gt;\n  &lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n  &lt;groupId&gt;mysql&lt;/groupId&gt;\n  &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n  &lt;version&gt;5.1.42&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n  &lt;groupId&gt;com.jfinal&lt;/groupId&gt;\n  &lt;artifactId&gt;jfinal&lt;/artifactId&gt;\n  &lt;version&gt;3.3&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n  &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n  &lt;artifactId&gt;druid&lt;/artifactId&gt;\n  &lt;version&gt;3.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre><p>主要用到的是上面这几个maven dependency</p>\n<h5 id=\"h5-u76EEu5F55u7ED3u6784\"><a name=\"目录结构\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>目录结构</h5><p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-1cc3f2edc210a5ee?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h5 id=\"h5-u6D4Bu8BD5u6548u679C\"><a name=\"测试效果\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>测试效果</h5><p>Controller类方法定义</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-ec09650e4ceef69a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Controller类方法定义\"></p>\n<p>swagger接口测试</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-9ddf0607c71e9c49?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"swagger接口测试\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-fb176102e54322dc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"swagger接口测试\"></p>\n<p>sql管理功能这里没贴上来，但我已经测试通过了，springboot打包成jar启动可以找到sql路径，之前我是使用PathKit.getRootPath这种方式，这种方式打包成jar后就找不到路径；按照<code>ActiveRecordPluginConfig类</code>配置即可</p>\n', ' #### 在写分享前先看了看jfinal-3.3的文档章节：5.13 任意环境下使用 ActiveRecordActiveRecordPlugin 可以独立于 java web 环境运行在任何普通的 java 程序中，使用方式极度简单，相对于 web 项目只需要手动调用一下其 start() 方法即可立即使用。\n##### 以下是代码示例：\n```\npublic class ActiveRecordTest{\n   public static void main(String[] args){\n       DruidPlugin dp=new DruidPlugin(\"localhost\",\"userName\",\"password\");\n       ActiveRecordPlugin arp=new ActiveRecordPlugin(dp);\n       arp.addMapping(\"blog\",Blog.class);\n       // 与web环境唯一的不同是要手动调用一次相关插件的start()方法\n       dp.start();\n       arp.start();\n       // 通过上面简单的几行代码，即可立即开始使用\n      newBlog().set(\"title\",\"title\").set(\"content\",\"cxt text\").save();\n       Blog.dao.findById(123);\n   }\n}\n```\n注意：``ActiveRecordPlugin`` 所依赖的其它插件也必须手动调用一下 start()方法，如上例中的``dp.start()``\n\n#####下面进入正题：\n\n创建一个插件类\n###### ActiveRecordPluginConfig类\n\n```\npackage com.choxsu.elastic.config;\nimport com.alibaba.druid.filter.stat.StatFilter;\nimport com.alibaba.druid.wall.WallFilter;\nimport com.choxsu.elastic.entity._MappingKit;\nimport com.jfinal.plugin.activerecord.ActiveRecordPlugin;\nimport com.jfinal.plugin.druid.DruidPlugin;\nimport com.jfinal.template.source.ClassPathSourceFactory;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport java.sql.Connection;\n/**\n * @author chox su\n * @date 2017/11/29 10:16\n */\n @Configuration\n public class ActiveRecordPluginConfig{\n    @Value(\"${spring.datasource.username}\")\n    private String username;\n\n    @Value(\"${spring.datasource.password}\")\n    private String password;\n\n    @Value(\"${spring.datasource.url}\")\n    private String url;\n\n    @Bean\n    public ActiveRecordPlug ininitActiveRecordPlugin(){\n        DruidPlugindruidPlugin=newDruidPlugin(url,username,password);\n        // 加强数据库安全\n        WallFilterwallFilter=newWallFilter();\n        wallFilter.setDbType(\"mysql\");\n        druidPlugin.addFilter(wallFilter);\n        // 添加 StatFilter 才会有统计数据\n        // druidPlugin.addFilter(new StatFilter());\n        // 必须手动调用start\n        druidPlugin.start();\n        ActiveRecordPlugin arp=new ActiveRecordPlugin(druidPlugin);\n        arp.setTransactionLevel(Connection.TRANSACTION_READ_COMMITTED);\n        _MappingKit.mapping(arp);\n        arp.setShowSql(false);\n        arp.getEngine().setSourceFactory(new ClassPathSourceFactory());\n        arp.addSqlTemplate(\"/sql/all_sqls.sql\");\n        // 必须手动调用start\n        arp.start();\n        return arp;\n    }\n}\n\n```\n###### application.yml配置文件\n```\nserver:\n  port: 1013\nspring:\n  application:\n    name: name:elastic\n  datasource:\n    driver-class-name: com.mysql.jdbc.Driver\n    username: root\n    password: root\n    url: jdbc:mysql://192.168.3.44:3306/jfinal_club?characterEncoding=utf8&useSSL=false\n```\n###### pom.xml配置文件\n```\n<dependency>\n  <groupId>com.zgxl.market</groupId>\n  <artifactId>fastjson</artifactId>\n  <version>1.0.0</version>\n</dependency>\n\n<dependency>\n  <groupId>mysql</groupId>\n  <artifactId>mysql-connector-java</artifactId>\n  <version>5.1.42</version>\n</dependency>\n\n<dependency>\n  <groupId>com.jfinal</groupId>\n  <artifactId>jfinal</artifactId>\n  <version>3.3</version>\n</dependency>\n\n<dependency>\n  <groupId>com.alibaba</groupId>\n  <artifactId>druid</artifactId>\n  <version>3.3</version>\n</dependency>\n\n```\n主要用到的是上面这几个maven dependency\n\n##### 目录结构\n\n![image](http://upload-images.jianshu.io/upload_images/7463793-1cc3f2edc210a5ee?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n##### 测试效果\n\nController类方法定义\n\n![Controller类方法定义](http://upload-images.jianshu.io/upload_images/7463793-ec09650e4ceef69a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nswagger接口测试\n\n![swagger接口测试](http://upload-images.jianshu.io/upload_images/7463793-9ddf0607c71e9c49?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![swagger接口测试](http://upload-images.jianshu.io/upload_images/7463793-fb176102e54322dc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nsql管理功能这里没贴上来，但我已经测试通过了，springboot打包成jar启动可以找到sql路径，之前我是使用PathKit.getRootPath这种方式，这种方式打包成jar后就找不到路径；按照``ActiveRecordPluginConfig类``配置即可\n', '2018-06-14 17:05:40', '2018-06-14 17:05:40', 20, 0, 0, 'blog', 0, 20, NULL);
INSERT INTO `blog` VALUES (26, 1, 'IDEA下利用Jrebel插件实现JFinal或SpringBoot【热启动】（Tomcat同理）', '<h4 id=\"h4--jrebel-\"><a name=\"第一步：下载jrebel插件，这个是必须的，步骤如下\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>第一步：下载jrebel插件，这个是必须的，步骤如下</h4><p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-a418deea2360d048.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image.png\"></p>\n<p>我这里已经下载了，如果没有下载，就选择下载，下载有点慢，慢慢等待就好，需要离线插件包可以@我</p>\n<p>下载完成后重启，然后激活，<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-872042626f586429.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>进入激活页面<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-495f4dd29f3d5610.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>也可以选择我的方式，去官网注册获取激活code</p>\n<p>具体激活参考这个帖子IntelliJ IDEA / JRebel 激活;</p>\n<p>jrebel相关设置</p>\n<p>file&gt;setting&gt;jrebel, 类资源刷新间隔设置<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-de410dbc8291f073.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>file&gt;setting&gt;jrebel&gt;Startup,默认即可<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-389b4fda86e9b426.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>第二步：进入正题，相应的设置，见图流程走下去</p>\n<p>使用快捷键ctrl+alt+shift+/ —&gt;registry<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-4ab5bec8da9a3e76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"imgage\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-1628ec688c372f89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"img\"></p>\n<p>到这里离热启动又近一步了</p>\n<p>打开设置，打开自动编译<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-3c9fcf100c16089f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>JRebel项目或者模块选择热部署页面<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-282b900c8c48ba68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>需要部署的项目勾选</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-63dd88070fa4de6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"><br>最后一步</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-9358147d45d8a4cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"><br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-3d5129496ec26d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"IMAGE\"><br>对比</p>\n<p>修改前</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-70c036ff59102922.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"IMAGE\"><br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-ae796611a61bae4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"IMAGE\"><br>修改后<br><img src=\"https://upload-images.jianshu.io/upload_images/7463793-5906f4d372963c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"IMAGE\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-5906f4d372963c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"IMAGE\"></p>\n<p>到这里就可以节约n多时间</p>\n', '#### 第一步：下载jrebel插件，这个是必须的，步骤如下\n![image.png](https://upload-images.jianshu.io/upload_images/7463793-a418deea2360d048.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n我这里已经下载了，如果没有下载，就选择下载，下载有点慢，慢慢等待就好，需要离线插件包可以@我\n\n下载完成后重启，然后激活，\n![image](https://upload-images.jianshu.io/upload_images/7463793-872042626f586429.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n进入激活页面\n![image](https://upload-images.jianshu.io/upload_images/7463793-495f4dd29f3d5610.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n也可以选择我的方式，去官网注册获取激活code\n\n具体激活参考这个帖子IntelliJ IDEA / JRebel 激活;\n\njrebel相关设置\n\nfile>setting>jrebel, 类资源刷新间隔设置\n![image](https://upload-images.jianshu.io/upload_images/7463793-de410dbc8291f073.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nfile>setting>jrebel>Startup,默认即可\n![image](https://upload-images.jianshu.io/upload_images/7463793-389b4fda86e9b426.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n第二步：进入正题，相应的设置，见图流程走下去\n\n使用快捷键ctrl+alt+shift+/ -->registry\n![imgage](https://upload-images.jianshu.io/upload_images/7463793-4ab5bec8da9a3e76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![img](https://upload-images.jianshu.io/upload_images/7463793-1628ec688c372f89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n到这里离热启动又近一步了\n\n打开设置，打开自动编译\n![image](https://upload-images.jianshu.io/upload_images/7463793-3c9fcf100c16089f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nJRebel项目或者模块选择热部署页面\n![image](https://upload-images.jianshu.io/upload_images/7463793-282b900c8c48ba68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n需要部署的项目勾选\n\n![image](https://upload-images.jianshu.io/upload_images/7463793-63dd88070fa4de6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n最后一步\n\n![image](https://upload-images.jianshu.io/upload_images/7463793-9358147d45d8a4cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n![IMAGE](https://upload-images.jianshu.io/upload_images/7463793-3d5129496ec26d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n对比\n\n修改前\n\n![IMAGE](https://upload-images.jianshu.io/upload_images/7463793-70c036ff59102922.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n![IMAGE](https://upload-images.jianshu.io/upload_images/7463793-ae796611a61bae4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n修改后\n![IMAGE](https://upload-images.jianshu.io/upload_images/7463793-5906f4d372963c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![IMAGE](https://upload-images.jianshu.io/upload_images/7463793-5906f4d372963c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n到这里就可以节约n多时间', '2018-06-14 17:14:52', '2018-06-14 17:14:52', 10, 0, 0, 'blog', 0, 1, NULL);
INSERT INTO `blog` VALUES (27, 1, 'IDEA push to origin/master was rejected错误解决方案', '<h5 id=\"h5-idea-oschina-git-git-push-push-to-origin-master-war-rejected-\"><a name=\"idea中，发布项目到OSChina的Git中，当时按照这样的流程添加Git，然后push，提示：push to origin/master war rejected”。\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>idea中，发布项目到OSChina的Git中，当时按照这样的流程添加Git，然后push，提示：push to origin/master war rejected”。</h5><p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-862e7460d27441a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-9c1cfecd15816596?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>解决方案如下：</p>\n<p>1.切换到自己项目所在的目录，右键选择<code>GIT BASH Here</code></p>\n<p>2.在terminl窗口中依次输入命令：</p>\n<pre><code>git pull\n\ngit pull origin master\n\ngit pull origin master --allow-unrelated-histories\n</code></pre><p>3.在idea中重新push自己的项目，成功！！！</p>\n<p>转自：<a href=\"http://push%20to%20origin/master%20was%20rejected%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\">http://blog.csdn.net/a137151062/article/details/78820806</a></p>\n', ' ##### idea中，发布项目到OSChina的Git中，当时按照这样的流程添加Git，然后push，提示：push to origin/master war rejected\"。\n\n![image](http://upload-images.jianshu.io/upload_images/7463793-862e7460d27441a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![image](http://upload-images.jianshu.io/upload_images/7463793-9c1cfecd15816596?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n解决方案如下：\n\n1.切换到自己项目所在的目录，右键选择`GIT BASH Here`\n\n2.在terminl窗口中依次输入命令：\n\n```\ngit pull\n\ngit pull origin master\n\ngit pull origin master --allow-unrelated-histories\n```\n\n3.在idea中重新push自己的项目，成功！！！\n\n转自：[http://blog.csdn.net/a137151062/article/details/78820806](http://push%20to%20origin/master%20was%20rejected%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88)\n', '2018-06-14 17:16:36', '2018-06-14 17:16:36', 10, 0, 0, 'blog', 0, 1, NULL);
INSERT INTO `blog` VALUES (28, 2, '详解log4j2(上) - 从基础到实战(转)', '<p> log4j2相对于log4j 1.x有了脱胎换骨的变化，其官网宣称的优势有多线程下10几倍于log4j 1.x和logback的高吞吐量、可配置的审计型日志、基于插件架构的各种灵活配置等。如果已经掌握log4j 1.x，使用log4j2还是非常简单的。</p>\n<p>先看一个示例</p>\n<h4 id=\"h4-1-\"><a name=\"1 基础配置\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1 基础配置</h4><p>普通java项目手动添加jar包</p>\n<pre><code>log4j-api-2.5.jar  \nlog4j-core-2.5.jar\n</code></pre><p>Maven项目pom.xml</p>\n<pre><code>&lt;dependencies&gt;  \n    &lt;dependency&gt;  \n        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;  \n        &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;  \n        &lt;version&gt;2.5&lt;/version&gt;  \n    &lt;/dependency&gt;  \n    &lt;dependency&gt;  \n        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;  \n        &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;  \n        &lt;version&gt;2.5&lt;/version&gt;  \n    &lt;/dependency&gt;  \n&lt;/dependencies&gt;\n</code></pre><p>测试代码</p>\n<pre><code>public static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(LogManager.ROOT_LOGGER_NAME);  \n    logger.trace(&quot;trace level&quot;);  \n    logger.debug(&quot;debug level&quot;);  \n    logger.info(&quot;info level&quot;);  \n    logger.warn(&quot;warn level&quot;);  \n    logger.error(&quot;error level&quot;);  \n    logger.fatal(&quot;fatal level&quot;);  \n}\n</code></pre><p>运行后输出</p>\n<pre><code>1.  ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.  \n2.  20:37:11.965 [main] ERROR  - error level  \n3.  20:37:11.965 [main] FATAL  - fatal level\n</code></pre><p>可以看到log4j2先发了一句牢骚，抱怨没有找到配置文件什么的，不过还是输出了error和fatal两个级别的信息。<br>log4j2默认会在classpath目录下寻找log4j.json、log4j.jsn、log4j2.xml等名称的文件，如果都没有找到，则会按默认配置输出，也就是输出到控制台。</p>\n<p>下面我们按默认配置添加一个log4j2.xml，添加到src根目录即可</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;  \n&lt;Configuration status=&quot;WARN&quot;&gt;  \n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n    &lt;/Appenders&gt;  \n    &lt;Loggers&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p>重新执行测试代码，可以看到输出结果相同，但是没有再提示找不到配置文件。<br>来看我们添加的配置文件log4j2.xml，以Configuration为根节点，有一个status属性，这个属性表示log4j2本身的日志信息打印级别。如果把status改为TRACE再执行测试代码，可以看到控制台中打印了一些log4j加载插件、组装logger等调试信息。<br>日志级别从低到高分为TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，如果设置为WARN，则低于WARN的信息都不会输出。对于Loggers中level的定义同样适用。</p>\n<p>下面是Appender配置，Appender可以理解为日志的输出目的地，这里配置了一个类型为Console的Appender，也就是输出到控制台。Console节点中的PatternLayout定义了输出日志时的格式：</p>\n<p>%d{HH:mm:ss.SSS} 表示输出到毫秒的时间</p>\n<p>%t 输出当前线程名称</p>\n<p>%-5level 输出日志级别，-5表示左对齐并且固定输出5个字符，如果不足在右边补0</p>\n<p>%logger 输出logger名称，因为Root Logger没有名称，所以没有输出</p>\n<p>%msg 日志文本</p>\n<p>%n 换行</p>\n<p>其他常用的占位符有：</p>\n<p>%F 输出所在的类文件名，如Client.java</p>\n<p>%L 输出行号</p>\n<p>%M 输出所在方法名</p>\n<p>%l  输出语句所在的行数, 包括类名、方法名、文件名、行数</p>\n<p>最后是Logger的配置，这里只配置了一个Root Logger。</p>\n<h4 id=\"h4-2-logger\"><a name=\"2 自定义Logger\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>2 自定义Logger</h4><p>首先修改测试代码</p>\n<pre><code>public static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(&quot;mylog&quot;);  \n    logger.trace(&quot;trace level&quot;);  \n    logger.debug(&quot;debug level&quot;);  \n    logger.info(&quot;info level&quot;);  \n    logger.warn(&quot;warn level&quot;);  \n    logger.error(&quot;error level&quot;);  \n    logger.fatal(&quot;fatal level&quot;);  \n}\n</code></pre><p>logger从获取Root Logger改为尝试获得一个名称为mylog的Logger，在配置文件中一无所得后，又找到了Root Logger，所以执行结果相同。<br>下面修改配置文件</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n    &lt;/Appenders&gt;  \n    &lt;Loggers&gt;  \n        &lt;Logger name=&quot;mylog&quot; level=&quot;trace&quot; additivity=&quot;false&quot;&gt;  \n        &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n    &lt;/Logger&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p>再次执行测试代码，这一次log4j2很高兴的找到了名称为mylog的配置，于是使用新配置把level改为trace，全部的信息都输出了。<br>additivity=”false”表示在该logger中输出的日志不会再延伸到父层logger。这里如果改为true，则会延伸到Root Logger，遵循Root Logger的配置也输出一次。</p>\n<p>注意根节点增加了一个monitorInterval属性，含义是每隔300秒重新读取配置文件，可以不重启应用的情况下修改配置，还是很好用的功能。</p>\n<h4 id=\"h4-3-appender\"><a name=\"3 自定义Appender\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>3 自定义Appender</h4><p>修改配置文件，添加一个文件类型的Appender，并且把mylog的AppenderRef改为新加的Appender</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n        &lt;File name=&quot;MyFile&quot; fileName=&quot;D:/logs/app.log&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/File&gt;  \n    &lt;/Appenders&gt;  \n    &lt;Loggers&gt;  \n        &lt;Logger name=&quot;mylog&quot; level=&quot;trace&quot; additivity=&quot;true&quot;&gt;  \n            &lt;AppenderRef ref=&quot;MyFile&quot; /&gt;  \n        &lt;/Logger&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p>执行并查看控制台和 <strong>D:/logs/app.log</strong>的输出结果</p>\n<h4 id=\"h4-4-\"><a name=\"4 实用型配置\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>4 实用型配置</h4><p>下面配置一个按时间和文件大小滚动的RollingRandomAccessFile Appender，名字真是够长，但不光只是名字长，相比RollingFileAppender有很大的性能提升，官网宣称是20-200%。</p>\n<p>Rolling的意思是当满足一定条件后，就重命名原日志文件用于备份，并从新生成一个新的日志文件。例如需求是每天生成一个日志文件，但是如果一天内的日志文件体积已经超过1G，就从新生成，两个条件满足一个即可。这在log4j 1.x原生功能中无法实现，在log4j2中就很简单了。</p>\n<p>看下面的配置</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;properties&gt;  \n        &lt;property name=&quot;LOG_HOME&quot;&gt;D:/logs&lt;/property&gt;  \n        &lt;property name=&quot;FILE_NAME&quot;&gt;mylog&lt;/property&gt;  \n    &lt;/properties&gt;  \n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n        &lt;RollingRandomAccessFile name=&quot;MyFile&quot;  \n            fileName=&quot;${LOG_HOME}/${FILE_NAME}.log&quot;  \n            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i.log&quot;&gt;  \n            &lt;PatternLayout  \n                pattern=&quot;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n            &lt;Policies&gt;  \n                &lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;  \n                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;  \n            &lt;/Policies&gt;  \n            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;  \n        &lt;/RollingRandomAccessFile&gt;  \n    &lt;/Appenders&gt;  \n\n    &lt;Loggers&gt;  \n        &lt;Logger name=&quot;mylog&quot; level=&quot;trace&quot; additivity=&quot;false&quot;&gt;  \n            &lt;AppenderRef ref=&quot;MyFile&quot; /&gt;  \n        &lt;/Logger&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p>&lt;properties&gt;定义了两个常量方便后面复用</p>\n<p>RollingRandomAccessFile的属性：</p>\n<p>fileName  指定当前日志文件的位置和文件名称</p>\n<p>filePattern  指定当发生Rolling时，文件的转移和重命名规则</p>\n<p>SizeBasedTriggeringPolicy  指定当文件体积大于size指定的值时，触发Rolling</p>\n<p>DefaultRolloverStrategy  指定最多保存的文件个数</p>\n<p>TimeBasedTriggeringPolicy  这个配置需要和filePattern结合使用，注意filePattern中配置的文件重命名规则是${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i，最小的时间粒度是mm，即分钟，TimeBasedTriggeringPolicy指定的size是1，结合起来就是每1分钟生成一个新文件。如果改成%d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件。</p>\n<p>修改测试代码，模拟文件体积超过10M和时间超过1分钟来验证结果</p>\n<pre><code>public static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(&quot;mylog&quot;);  \n    for(int i = 0; i &lt; 50000; i++) {  \n        logger.trace(&quot;trace level&quot;);  \n        logger.debug(&quot;debug level&quot;);  \n        logger.info(&quot;info level&quot;);  \n        logger.warn(&quot;warn level&quot;);  \n        logger.error(&quot;error level&quot;);  \n        logger.fatal(&quot;fatal level&quot;);  \n    }  \n    try {  \n        Thread.sleep(1000 * 61);  \n    } catch (InterruptedException e) {}  \n    logger.trace(&quot;trace level&quot;);  \n    logger.debug(&quot;debug level&quot;);  \n    logger.info(&quot;info level&quot;);  \n    logger.warn(&quot;warn level&quot;);  \n    logger.error(&quot;error level&quot;);  \n    logger.fatal(&quot;fatal level&quot;);  \n}\n</code></pre><h4 id=\"h4-5-\"><a name=\"5 自定义配置文件位置\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>5 自定义配置文件位置</h4><p>log4j2默认在classpath下查找配置文件，可以修改配置文件的位置。在非web项目中：</p>\n<pre><code>public static void main(String[] args) throws IOException {  \n    File file = new File(&quot;D:/log4j2.xml&quot;);  \n    BufferedInputStream in = new BufferedInputStream(new FileInputStream(file));  \n    final ConfigurationSource source = new ConfigurationSource(in);  \n    Configurator.initialize(null, source);  \n\n    Logger logger = LogManager.getLogger(&quot;mylog&quot;);  \n}\n</code></pre><p>如果是web项目，在web.xml中添加</p>\n<pre><code>&lt;context-param&gt;  \n    &lt;param-name&gt;log4jConfiguration&lt;/param-name&gt;  \n    &lt;param-value&gt;/WEB-INF/conf/log4j2.xml&lt;/param-value&gt;  \n&lt;/context-param&gt;  \n\n&lt;listener&gt;  \n    &lt;listener-class&gt;org.apache.logging.log4j.web.Log4jServletContextListener&lt;/listener-class&gt;  \n&lt;/listener&gt;\n</code></pre><p>掌握这些基本可以实际使用了，下篇介绍一些高级应用，异步Appender、MongoDB Appender和基于Filters的按级别输出到不同文件的设置</p>\n<p><em>转自：</em> <a href=\"http://blog.csdn.net/autfish/article/details/51203709\">http://blog.csdn.net/autfish/article/details/51203709</a></p>\n', ' log4j2相对于log4j 1.x有了脱胎换骨的变化，其官网宣称的优势有多线程下10几倍于log4j 1.x和logback的高吞吐量、可配置的审计型日志、基于插件架构的各种灵活配置等。如果已经掌握log4j 1.x，使用log4j2还是非常简单的。\n\n先看一个示例\n####1 基础配置\n普通java项目手动添加jar包\n\n```\nlog4j-api-2.5.jar  \nlog4j-core-2.5.jar  \n```\nMaven项目pom.xml\n\n```\n<dependencies>  \n    <dependency>  \n        <groupId>org.apache.logging.log4j</groupId>  \n        <artifactId>log4j-api</artifactId>  \n        <version>2.5</version>  \n    </dependency>  \n    <dependency>  \n        <groupId>org.apache.logging.log4j</groupId>  \n        <artifactId>log4j-core</artifactId>  \n        <version>2.5</version>  \n    </dependency>  \n</dependencies>  \n```\n测试代码\n```\npublic static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(LogManager.ROOT_LOGGER_NAME);  \n    logger.trace(\"trace level\");  \n    logger.debug(\"debug level\");  \n    logger.info(\"info level\");  \n    logger.warn(\"warn level\");  \n    logger.error(\"error level\");  \n    logger.fatal(\"fatal level\");  \n}  \n```\n运行后输出\n```\n1.  ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.  \n2.  20:37:11.965 [main] ERROR  - error level  \n3.  20:37:11.965 [main] FATAL  - fatal level\n\n```\n可以看到log4j2先发了一句牢骚，抱怨没有找到配置文件什么的，不过还是输出了error和fatal两个级别的信息。\nlog4j2默认会在classpath目录下寻找log4j.json、log4j.jsn、log4j2.xml等名称的文件，如果都没有找到，则会按默认配置输出，也就是输出到控制台。\n\n下面我们按默认配置添加一个log4j2.xml，添加到src根目录即可\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<Configuration status=\"WARN\">  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n    </Appenders>  \n    <Loggers>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n重新执行测试代码，可以看到输出结果相同，但是没有再提示找不到配置文件。\n来看我们添加的配置文件log4j2.xml，以Configuration为根节点，有一个status属性，这个属性表示log4j2本身的日志信息打印级别。如果把status改为TRACE再执行测试代码，可以看到控制台中打印了一些log4j加载插件、组装logger等调试信息。\n日志级别从低到高分为TRACE < DEBUG < INFO < WARN < ERROR < FATAL，如果设置为WARN，则低于WARN的信息都不会输出。对于Loggers中level的定义同样适用。\n\n下面是Appender配置，Appender可以理解为日志的输出目的地，这里配置了一个类型为Console的Appender，也就是输出到控制台。Console节点中的PatternLayout定义了输出日志时的格式：\n\n%d{HH:mm:ss.SSS} 表示输出到毫秒的时间\n\n%t 输出当前线程名称\n\n%-5level 输出日志级别，-5表示左对齐并且固定输出5个字符，如果不足在右边补0\n\n%logger 输出logger名称，因为Root Logger没有名称，所以没有输出\n\n%msg 日志文本\n\n%n 换行\n\n其他常用的占位符有：\n\n%F 输出所在的类文件名，如Client.java\n\n%L 输出行号\n\n%M 输出所在方法名\n\n%l  输出语句所在的行数, 包括类名、方法名、文件名、行数\n\n最后是Logger的配置，这里只配置了一个Root Logger。\n####2 自定义Logger\n\n首先修改测试代码\n\n```\npublic static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(\"mylog\");  \n    logger.trace(\"trace level\");  \n    logger.debug(\"debug level\");  \n    logger.info(\"info level\");  \n    logger.warn(\"warn level\");  \n    logger.error(\"error level\");  \n    logger.fatal(\"fatal level\");  \n}  \n```\nlogger从获取Root Logger改为尝试获得一个名称为mylog的Logger，在配置文件中一无所得后，又找到了Root Logger，所以执行结果相同。\n下面修改配置文件\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n    </Appenders>  \n    <Loggers>  \n        <Logger name=\"mylog\" level=\"trace\" additivity=\"false\">  \n        <AppenderRef ref=\"Console\" />  \n    </Logger>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n再次执行测试代码，这一次log4j2很高兴的找到了名称为mylog的配置，于是使用新配置把level改为trace，全部的信息都输出了。\nadditivity=\"false\"表示在该logger中输出的日志不会再延伸到父层logger。这里如果改为true，则会延伸到Root Logger，遵循Root Logger的配置也输出一次。\n\n注意根节点增加了一个monitorInterval属性，含义是每隔300秒重新读取配置文件，可以不重启应用的情况下修改配置，还是很好用的功能。\n####3 自定义Appender\n修改配置文件，添加一个文件类型的Appender，并且把mylog的AppenderRef改为新加的Appender\n\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n        <File name=\"MyFile\" fileName=\"D:/logs/app.log\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </File>  \n    </Appenders>  \n    <Loggers>  \n        <Logger name=\"mylog\" level=\"trace\" additivity=\"true\">  \n            <AppenderRef ref=\"MyFile\" />  \n        </Logger>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>\n```\n执行并查看控制台和 **D:/logs/app.log**的输出结果\n\n####4 实用型配置\n下面配置一个按时间和文件大小滚动的RollingRandomAccessFile Appender，名字真是够长，但不光只是名字长，相比RollingFileAppender有很大的性能提升，官网宣称是20-200%。\n\nRolling的意思是当满足一定条件后，就重命名原日志文件用于备份，并从新生成一个新的日志文件。例如需求是每天生成一个日志文件，但是如果一天内的日志文件体积已经超过1G，就从新生成，两个条件满足一个即可。这在log4j 1.x原生功能中无法实现，在log4j2中就很简单了。\n\n看下面的配置\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <properties>  \n        <property name=\"LOG_HOME\">D:/logs</property>  \n        <property name=\"FILE_NAME\">mylog</property>  \n    </properties>  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n        <RollingRandomAccessFile name=\"MyFile\"  \n            fileName=\"${LOG_HOME}/${FILE_NAME}.log\"  \n            filePattern=\"${LOG_HOME}/$${date:yyyy-MM}/${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i.log\">  \n            <PatternLayout  \n                pattern=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n            <Policies>  \n                <TimeBasedTriggeringPolicy interval=\"1\" />  \n                <SizeBasedTriggeringPolicy size=\"10 MB\" />  \n            </Policies>  \n            <DefaultRolloverStrategy max=\"20\" />  \n        </RollingRandomAccessFile>  \n    </Appenders>  \n  \n    <Loggers>  \n        <Logger name=\"mylog\" level=\"trace\" additivity=\"false\">  \n            <AppenderRef ref=\"MyFile\" />  \n        </Logger>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n<properties>定义了两个常量方便后面复用\n\nRollingRandomAccessFile的属性：\n\nfileName  指定当前日志文件的位置和文件名称\n\nfilePattern  指定当发生Rolling时，文件的转移和重命名规则\n\nSizeBasedTriggeringPolicy  指定当文件体积大于size指定的值时，触发Rolling\n\nDefaultRolloverStrategy  指定最多保存的文件个数\n\nTimeBasedTriggeringPolicy  这个配置需要和filePattern结合使用，注意filePattern中配置的文件重命名规则是${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i，最小的时间粒度是mm，即分钟，TimeBasedTriggeringPolicy指定的size是1，结合起来就是每1分钟生成一个新文件。如果改成%d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件。\n\n修改测试代码，模拟文件体积超过10M和时间超过1分钟来验证结果\n\n```\npublic static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(\"mylog\");  \n    for(int i = 0; i < 50000; i++) {  \n        logger.trace(\"trace level\");  \n        logger.debug(\"debug level\");  \n        logger.info(\"info level\");  \n        logger.warn(\"warn level\");  \n        logger.error(\"error level\");  \n        logger.fatal(\"fatal level\");  \n    }  \n    try {  \n        Thread.sleep(1000 * 61);  \n    } catch (InterruptedException e) {}  \n    logger.trace(\"trace level\");  \n    logger.debug(\"debug level\");  \n    logger.info(\"info level\");  \n    logger.warn(\"warn level\");  \n    logger.error(\"error level\");  \n    logger.fatal(\"fatal level\");  \n}  \n```\n####5 自定义配置文件位置\nlog4j2默认在classpath下查找配置文件，可以修改配置文件的位置。在非web项目中：\n```\npublic static void main(String[] args) throws IOException {  \n    File file = new File(\"D:/log4j2.xml\");  \n    BufferedInputStream in = new BufferedInputStream(new FileInputStream(file));  \n    final ConfigurationSource source = new ConfigurationSource(in);  \n    Configurator.initialize(null, source);  \n      \n    Logger logger = LogManager.getLogger(\"mylog\");  \n} \n ```\n如果是web项目，在web.xml中添加\n```\n<context-param>  \n    <param-name>log4jConfiguration</param-name>  \n    <param-value>/WEB-INF/conf/log4j2.xml</param-value>  \n</context-param>  \n  \n<listener>  \n    <listener-class>org.apache.logging.log4j.web.Log4jServletContextListener</listener-class>  \n</listener>  \n```\n掌握这些基本可以实际使用了，下篇介绍一些高级应用，异步Appender、MongoDB Appender和基于Filters的按级别输出到不同文件的设置\n\n*转自：* [http://blog.csdn.net/autfish/article/details/51203709](http://blog.csdn.net/autfish/article/details/51203709)\n ', '2018-06-14 18:18:09', '2018-06-14 17:17:09', 22, 0, 0, 'favorite', 0, 1, NULL);
INSERT INTO `blog` VALUES (29, 2, '详解log4j2(下) - Async/MongoDB/Flume Appender 按日志级别区分文件输出', '<h3 id=\"h3-1-\"><a name=\"1. 按日志级别区分文件输出\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1. 按日志级别区分文件输出</h3><p>有些人习惯按日志信息级别输出到不同名称的文件中，如info.log，error.log，warn.log等，在log4j2中可通过配置Filters来实现。</p>\n<p>假定需求是把INFO及以下级别的信息输出到info.log，WARN和ERROR级别的信息输出到error.log，FATAL级别输出到fatal.log，配置文件如下：</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;\n    &lt;properties&gt;\n        &lt;property name=&quot;LOG_HOME&quot;&gt;D:/logs&lt;/property&gt;\n    &lt;/properties&gt;\n    &lt;Appenders&gt;\n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;\n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;\n        &lt;/Console&gt;\n\n        &lt;RollingRandomAccessFile name=&quot;InfoFile&quot;\n            fileName=&quot;${LOG_HOME}/info.log&quot;\n            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/info-%d{yyyy-MM-dd}-%i.log&quot;&gt;\n            &lt;Filters&gt;\n                &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;DENY&quot; onMismatch=&quot;NEUTRAL&quot; /&gt;\n                &lt;ThresholdFilter level=&quot;trace&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot; /&gt;\n            &lt;/Filters&gt;\n            &lt;PatternLayout pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;\n            &lt;Policies&gt;\n                &lt;TimeBasedTriggeringPolicy /&gt;\n                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;\n            &lt;/Policies&gt;\n            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;\n        &lt;/RollingRandomAccessFile&gt;\n\n        &lt;RollingRandomAccessFile name=&quot;ErrorFile&quot;\n            fileName=&quot;${LOG_HOME}/error.log&quot;\n            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/error-%d{yyyy-MM-dd}-%i.log&quot;&gt;\n            &lt;Filters&gt;\n                &lt;ThresholdFilter level=&quot;fatal&quot; onMatch=&quot;DENY&quot; onMismatch=&quot;NEUTRAL&quot; /&gt;\n                &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot; /&gt;\n            &lt;/Filters&gt;\n            &lt;PatternLayout pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;\n            &lt;Policies&gt;\n                &lt;TimeBasedTriggeringPolicy /&gt;\n                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;\n            &lt;/Policies&gt;\n            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;\n        &lt;/RollingRandomAccessFile&gt;\n\n        &lt;RollingRandomAccessFile name=&quot;FatalFile&quot;\n            fileName=&quot;${LOG_HOME}/fatal.log&quot;\n            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/fatal-%d{yyyy-MM-dd}-%i.log&quot;&gt;\n            &lt;Filters&gt;\n                &lt;ThresholdFilter level=&quot;fatal&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot; /&gt;\n            &lt;/Filters&gt;\n            &lt;PatternLayout pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;\n            &lt;Policies&gt;\n                &lt;TimeBasedTriggeringPolicy /&gt;\n                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;\n            &lt;/Policies&gt;\n            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;\n        &lt;/RollingRandomAccessFile&gt;\n    &lt;/Appenders&gt;\n\n    &lt;Loggers&gt;\n        &lt;Root level=&quot;trace&quot;&gt;\n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;\n            &lt;AppenderRef ref=&quot;InfoFile&quot; /&gt;\n            &lt;AppenderRef ref=&quot;ErrorFile&quot; /&gt;\n            &lt;AppenderRef ref=&quot;FatalFile&quot; /&gt;\n        &lt;/Root&gt;\n    &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre><p>测试代码：</p>\n<pre><code>public static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(Client.class);  \n    logger.trace(&quot;trace level&quot;);  \n    logger.debug(&quot;debug level&quot;);  \n    logger.info(&quot;info level&quot;);  \n    logger.warn(&quot;warn level&quot;);  \n    logger.error(&quot;error level&quot;);  \n    logger.fatal(&quot;fatal level&quot;);  \n}\n</code></pre><h3 id=\"h3-2-\"><a name=\"2 异步写日志\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>2 异步写日志</h3><p>配置文件：</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;properties&gt;  \n        &lt;property name=&quot;LOG_HOME&quot;&gt;D:/logs&lt;/property&gt;  \n        &lt;property name=&quot;FILE_NAME&quot;&gt;mylog&lt;/property&gt;  \n    &lt;/properties&gt;  \n\n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n        &lt;RollingRandomAccessFile name=&quot;MyFile&quot;  \n            fileName=&quot;${LOG_HOME}/${FILE_NAME}.log&quot;  \n            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i.log&quot;&gt;  \n            &lt;PatternLayout  \n                pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;  \n            &lt;Policies&gt;  \n                &lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;  \n                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;  \n            &lt;/Policies&gt;  \n            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;  \n        &lt;/RollingRandomAccessFile&gt;  \n        &lt;Async name=&quot;Async&quot;&gt;  \n            &lt;AppenderRef ref=&quot;MyFile&quot; /&gt;  \n        &lt;/Async&gt;  \n    &lt;/Appenders&gt;  \n\n    &lt;Loggers&gt;  \n        &lt;Logger name=&quot;asynclog&quot; level=&quot;trace&quot; additivity=&quot;false&quot; &gt;  \n            &lt;AppenderRef ref=&quot;Async&quot; /&gt;  \n        &lt;/Logger&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p>测试代码：</p>\n<pre><code>public static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(&quot;asynclog&quot;);  \n    logger.trace(&quot;trace level&quot;);  \n    logger.debug(&quot;debug level&quot;);  \n    logger.info(&quot;info level&quot;);  \n    logger.warn(&quot;warn level&quot;);  \n    logger.error(&quot;error level&quot;);  \n    logger.fatal(&quot;fatal level&quot;);  \n}\n</code></pre><h3 id=\"h3-3-mongodb\"><a name=\"3 输出到MongoDB\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>3 输出到MongoDB</h3><p>添加依赖：</p>\n<pre><code>&lt;dependency&gt;  \n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;  \n    &lt;artifactId&gt;log4j-nosql&lt;/artifactId&gt;  \n    &lt;version&gt;2.5&lt;/version&gt;  \n&lt;/dependency&gt;  \n&lt;dependency&gt;  \n    &lt;groupId&gt;org.mongodb&lt;/groupId&gt;  \n    &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;  \n    &lt;version&gt;3.2.2&lt;/version&gt;  \n&lt;/dependency&gt;\n</code></pre><p>配置文件：</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;Appenders&gt;  \n        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;  \n            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;  \n        &lt;/Console&gt;  \n\n        &lt;NoSql name=&quot;databaseAppender&quot;&gt;  \n            &lt;MongoDb databaseName=&quot;test&quot; collectionName=&quot;errorlog&quot;  \n                server=&quot;localhost&quot; port=&quot;27017&quot; /&gt;  \n        &lt;/NoSql&gt;  \n    &lt;/Appenders&gt;  \n\n    &lt;Loggers&gt;  \n        &lt;Logger name=&quot;mongolog&quot; level=&quot;trace&quot; additivity=&quot;false&quot;&gt;  \n            &lt;AppenderRef ref=&quot;databaseAppender&quot; /&gt;  \n        &lt;/Logger&gt;  \n        &lt;Root level=&quot;error&quot;&gt;  \n            &lt;AppenderRef ref=&quot;Console&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><h3 id=\"h3-4-flume\"><a name=\"4 输出到Flume\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>4 输出到Flume</h3><p>Flume配置(flume-conf.properties)</p>\n<pre><code>agent1.sources=source1   \nagent1.sinks=sink1   \nagent1.channels=channel1   \n\nagent1.sources.source1.type=avro  \nagent1.sources.source1.channels=channel1  \nagent1.sources.source1.bind=0.0.0.0  \nagent1.sources.source1.port=41414  \n\nagent1.sinks.sink1.type=file_roll   \nagent1.sinks.sink1.sink.directory=D:/log  \nagent1.sinks.sink1.channel=channel1  \nagent1.sinks.sink1.sink.rollInterval=86400  \nagent1.sinks.sink1.sink.batchSize=100  \nagent1.sinks.sink1.sink.serializer=text  \nagent1.sinks.sink1.sink.serializer.appendNewline = false  \n\nagent1.channels.channel1.type=file   \nagent1.channels.channel1.checkpointDir=D:/log/checkpoint   \nagent1.channels.channel1.dataDirs=D:/log/data\n</code></pre><p>启动Flume（注：测试环境为windows）</p>\n<pre><code>flume-ng.cmd agent --conf ../conf/ --conf-file ../conf/flume-conf.properties -name agent1\n</code></pre><p>添加依赖：</p>\n<pre><code>&lt;dependency&gt;  \n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;  \n    &lt;artifactId&gt;log4j-flume-ng&lt;/artifactId&gt;  \n    &lt;version&gt;2.5&lt;/version&gt;  \n&lt;/dependency&gt;\n</code></pre><p>配置文件：</p>\n<pre><code>&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;  \n    &lt;Appenders&gt;  \n        &lt;Flume name=&quot;eventLogger&quot; compress=&quot;false&quot;&gt;  \n            &lt;Agent host=&quot;127.0.0.1&quot; port=&quot;41414&quot; /&gt;  \n            &lt;RFC5424Layout enterpriseNumber=&quot;18060&quot; includeMDC=&quot;true&quot; appName=&quot;MyApp&quot; /&gt;  \n        &lt;/Flume&gt;  \n    &lt;/Appenders&gt;  \n    &lt;Loggers&gt;  \n        &lt;Root level=&quot;trace&quot;&gt;  \n            &lt;AppenderRef ref=&quot;eventLogger&quot; /&gt;  \n        &lt;/Root&gt;  \n    &lt;/Loggers&gt;  \n&lt;/Configuration&gt;\n</code></pre><p><em>转自:</em><a href=\"http://blog.csdn.net/autfish/article/details/51244787\">http://blog.csdn.net/autfish/article/details/51244787</a></p>\n', ' ### 1. 按日志级别区分文件输出\n\n有些人习惯按日志信息级别输出到不同名称的文件中，如info.log，error.log，warn.log等，在log4j2中可通过配置Filters来实现。\n\n假定需求是把INFO及以下级别的信息输出到info.log，WARN和ERROR级别的信息输出到error.log，FATAL级别输出到fatal.log，配置文件如下：\n\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">\n	<properties>\n        <property name=\"LOG_HOME\">D:/logs</property>\n    </properties>\n	<Appenders>\n		<Console name=\"Console\" target=\"SYSTEM_OUT\">\n			<PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />\n		</Console>\n\n		<RollingRandomAccessFile name=\"InfoFile\"\n			fileName=\"${LOG_HOME}/info.log\"\n			filePattern=\"${LOG_HOME}/$${date:yyyy-MM}/info-%d{yyyy-MM-dd}-%i.log\">\n			<Filters>\n				<ThresholdFilter level=\"warn\" onMatch=\"DENY\" onMismatch=\"NEUTRAL\" />\n				<ThresholdFilter level=\"trace\" onMatch=\"ACCEPT\" onMismatch=\"DENY\" />\n			</Filters>\n			<PatternLayout pattern=\"%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n\" />\n			<Policies>\n				<TimeBasedTriggeringPolicy />\n				<SizeBasedTriggeringPolicy size=\"10 MB\" />\n			</Policies>\n			<DefaultRolloverStrategy max=\"20\" />\n		</RollingRandomAccessFile>\n		\n		<RollingRandomAccessFile name=\"ErrorFile\"\n			fileName=\"${LOG_HOME}/error.log\"\n			filePattern=\"${LOG_HOME}/$${date:yyyy-MM}/error-%d{yyyy-MM-dd}-%i.log\">\n			<Filters>\n				<ThresholdFilter level=\"fatal\" onMatch=\"DENY\" onMismatch=\"NEUTRAL\" />\n				<ThresholdFilter level=\"warn\" onMatch=\"ACCEPT\" onMismatch=\"DENY\" />\n			</Filters>\n			<PatternLayout pattern=\"%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n\" />\n			<Policies>\n				<TimeBasedTriggeringPolicy />\n				<SizeBasedTriggeringPolicy size=\"10 MB\" />\n			</Policies>\n			<DefaultRolloverStrategy max=\"20\" />\n		</RollingRandomAccessFile>\n		\n		<RollingRandomAccessFile name=\"FatalFile\"\n			fileName=\"${LOG_HOME}/fatal.log\"\n			filePattern=\"${LOG_HOME}/$${date:yyyy-MM}/fatal-%d{yyyy-MM-dd}-%i.log\">\n			<Filters>\n				<ThresholdFilter level=\"fatal\" onMatch=\"ACCEPT\" onMismatch=\"DENY\" />\n			</Filters>\n			<PatternLayout pattern=\"%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n\" />\n			<Policies>\n				<TimeBasedTriggeringPolicy />\n				<SizeBasedTriggeringPolicy size=\"10 MB\" />\n			</Policies>\n			<DefaultRolloverStrategy max=\"20\" />\n		</RollingRandomAccessFile>\n	</Appenders>\n\n	<Loggers>\n		<Root level=\"trace\">\n			<AppenderRef ref=\"Console\" />\n			<AppenderRef ref=\"InfoFile\" />\n			<AppenderRef ref=\"ErrorFile\" />\n			<AppenderRef ref=\"FatalFile\" />\n		</Root>\n	</Loggers>\n</Configuration>\n```\n\n测试代码：\n\n```\npublic static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(Client.class);  \n    logger.trace(\"trace level\");  \n    logger.debug(\"debug level\");  \n    logger.info(\"info level\");  \n    logger.warn(\"warn level\");  \n    logger.error(\"error level\");  \n    logger.fatal(\"fatal level\");  \n} \n```\n### 2 异步写日志\n\n配置文件：\n\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <properties>  \n        <property name=\"LOG_HOME\">D:/logs</property>  \n        <property name=\"FILE_NAME\">mylog</property>  \n    </properties>  \n  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n        <RollingRandomAccessFile name=\"MyFile\"  \n            fileName=\"${LOG_HOME}/${FILE_NAME}.log\"  \n            filePattern=\"${LOG_HOME}/$${date:yyyy-MM}/${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i.log\">  \n            <PatternLayout  \n                pattern=\"%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n\" />  \n            <Policies>  \n                <TimeBasedTriggeringPolicy interval=\"1\" />  \n                <SizeBasedTriggeringPolicy size=\"10 MB\" />  \n            </Policies>  \n            <DefaultRolloverStrategy max=\"20\" />  \n        </RollingRandomAccessFile>  \n        <Async name=\"Async\">  \n            <AppenderRef ref=\"MyFile\" />  \n        </Async>  \n    </Appenders>  \n  \n    <Loggers>  \n        <Logger name=\"asynclog\" level=\"trace\" additivity=\"false\" >  \n            <AppenderRef ref=\"Async\" />  \n        </Logger>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n测试代码：\n\n```\npublic static void main(String[] args) {  \n    Logger logger = LogManager.getLogger(\"asynclog\");  \n    logger.trace(\"trace level\");  \n    logger.debug(\"debug level\");  \n    logger.info(\"info level\");  \n    logger.warn(\"warn level\");  \n    logger.error(\"error level\");  \n    logger.fatal(\"fatal level\");  \n}  \n```\n\n\n### 3 输出到MongoDB\n\n添加依赖：\n\n```\n<dependency>  \n    <groupId>org.apache.logging.log4j</groupId>  \n    <artifactId>log4j-nosql</artifactId>  \n    <version>2.5</version>  \n</dependency>  \n<dependency>  \n    <groupId>org.mongodb</groupId>  \n    <artifactId>mongo-java-driver</artifactId>  \n    <version>3.2.2</version>  \n</dependency>  \n```\n\n\n配置文件：\n\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <Appenders>  \n        <Console name=\"Console\" target=\"SYSTEM_OUT\">  \n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" />  \n        </Console>  \n  \n        <NoSql name=\"databaseAppender\">  \n            <MongoDb databaseName=\"test\" collectionName=\"errorlog\"  \n                server=\"localhost\" port=\"27017\" />  \n        </NoSql>  \n    </Appenders>  \n  \n    <Loggers>  \n        <Logger name=\"mongolog\" level=\"trace\" additivity=\"false\">  \n            <AppenderRef ref=\"databaseAppender\" />  \n        </Logger>  \n        <Root level=\"error\">  \n            <AppenderRef ref=\"Console\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n\n\n### 4 输出到Flume\n\nFlume配置(flume-conf.properties)\n\n```\nagent1.sources=source1   \nagent1.sinks=sink1   \nagent1.channels=channel1   \n  \nagent1.sources.source1.type=avro  \nagent1.sources.source1.channels=channel1  \nagent1.sources.source1.bind=0.0.0.0  \nagent1.sources.source1.port=41414  \n  \nagent1.sinks.sink1.type=file_roll   \nagent1.sinks.sink1.sink.directory=D:/log  \nagent1.sinks.sink1.channel=channel1  \nagent1.sinks.sink1.sink.rollInterval=86400  \nagent1.sinks.sink1.sink.batchSize=100  \nagent1.sinks.sink1.sink.serializer=text  \nagent1.sinks.sink1.sink.serializer.appendNewline = false  \n  \nagent1.channels.channel1.type=file   \nagent1.channels.channel1.checkpointDir=D:/log/checkpoint   \nagent1.channels.channel1.dataDirs=D:/log/data  \n```\n\n启动Flume（注：测试环境为windows）\n\n```\nflume-ng.cmd agent --conf ../conf/ --conf-file ../conf/flume-conf.properties -name agent1  \n```\n\n添加依赖：\n\n```\n<dependency>  \n    <groupId>org.apache.logging.log4j</groupId>  \n    <artifactId>log4j-flume-ng</artifactId>  \n    <version>2.5</version>  \n</dependency> \n```\n\n配置文件：\n\n```\n<Configuration status=\"WARN\" monitorInterval=\"300\">  \n    <Appenders>  \n        <Flume name=\"eventLogger\" compress=\"false\">  \n            <Agent host=\"127.0.0.1\" port=\"41414\" />  \n            <RFC5424Layout enterpriseNumber=\"18060\" includeMDC=\"true\" appName=\"MyApp\" />  \n        </Flume>  \n    </Appenders>  \n    <Loggers>  \n        <Root level=\"trace\">  \n            <AppenderRef ref=\"eventLogger\" />  \n        </Root>  \n    </Loggers>  \n</Configuration>  \n```\n*转自:*[http://blog.csdn.net/autfish/article/details/51244787](http://blog.csdn.net/autfish/article/details/51244787)\n ', '2018-06-14 18:18:34', '2018-06-14 17:17:25', 41, 0, 0, 'favorite', 0, 1, NULL);
INSERT INTO `blog` VALUES (30, 1, '解决mysql数据库tinyInt(1) 转换为java的Boolean解决方案', '<p> JAVA数据类型 和 MYSQL的数据类型转换，要注意tinyInt 类型，且存储长度为1的情况。</p>\n<h3 id=\"h3-mysql-\"><a name=\"mysql文档给出的解释\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>mysql文档给出的解释</h3><p><code>java.lang.Boolean if the configuration property tinyInt1isBit is set to true (the default) and the storage size is 1, or java.lang.Integer if not.</code></p>\n<h3 id=\"h3-u8981u6CE8u610Fu4E0Bu9762u8FD9u4E2Au63D0u793A\"><a name=\"要注意下面这个提示\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>要注意下面这个提示</h3><p>The <code>ResultSet.getObject()</code> method uses the type conversions between MySQL and Java types, following the JDBC specification where appropriate. The values returned by <code>ResultSetMetaData.GetColumnTypeName()</code>and <code>ResultSetMetaData.GetColumnClassName()</code> are shown in the table below. For more information on the JDBC types, see the reference on the <a href=\"http://docs.oracle.com/javase/8/docs/api/java/sql/Types.html\">java.sql.Types</a> class.</p>\n<hr>\n<p>文档地址：<a href=\"https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-type-conversions.html\">https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-type-conversions.html</a></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-d7bfb2c46f32edd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image.png\"></p>\n<h3 id=\"h3-u89E3u51B3u65B9u6848\"><a name=\"解决方案\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>解决方案</h3><p>1、<code>tinyInt(1)</code> 只用来代表Boolean含义的字段，且0代表False，1代表True。如果要存储多个数值，则定义为<code>tinyInt(N), N&gt;1</code>。例如 <code>tinyInt(2)</code><br>2、JDBC的URL增加 tinyInt1isBit=false参数，注意参数名区分大小写，否则不生效</p>\n<pre><code>jdbc:mysql://${ucmha.proxy1_2.host}/${db.mysql.db}?tinyInt1isBit=false\n</code></pre>', ' JAVA数据类型 和 MYSQL的数据类型转换，要注意tinyInt 类型，且存储长度为1的情况。\n\n\n### mysql文档给出的解释\n`\njava.lang.Boolean if the configuration property tinyInt1isBit is set to true (the default) and the storage size is 1, or java.lang.Integer if not.\n`\n### 要注意下面这个提示\n\nThe `ResultSet.getObject()` method uses the type conversions between MySQL and Java types, following the JDBC specification where appropriate. The values returned by `ResultSetMetaData.GetColumnTypeName()`and `ResultSetMetaData.GetColumnClassName()` are shown in the table below. For more information on the JDBC types, see the reference on the [java.sql.Types](http://docs.oracle.com/javase/8/docs/api/java/sql/Types.html) class.\n\n------\n\n文档地址：https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-type-conversions.html\n\n![image.png](https://upload-images.jianshu.io/upload_images/7463793-d7bfb2c46f32edd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### 解决方案\n1、`tinyInt(1)` 只用来代表Boolean含义的字段，且0代表False，1代表True。如果要存储多个数值，则定义为`tinyInt(N), N>1`。例如 `tinyInt(2)`\n2、JDBC的URL增加 tinyInt1isBit=false参数，注意参数名区分大小写，否则不生效\n```\njdbc:mysql://${ucmha.proxy1_2.host}/${db.mysql.db}?tinyInt1isBit=false\n```\n\n', '2018-06-14 17:17:51', '2018-06-14 17:17:51', 17, 0, 0, 'blog', 0, 8, NULL);
INSERT INTO `blog` VALUES (31, 1, 'Guava教程之EventBus', '<p> <strong>EventBus</strong>允许在组件之间进行发布-订阅式的通信，而不需要组件之间显式地进行注册(从而相互了解)。它的设计完全是为了用显式注册代替传统的Java进程内事件分布。它不是通用的发布-订阅系统，也不是用于进程间通信的。</p>\n<h3 id=\"h3-eventbus-\"><a name=\"EventBus对象的创建\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>EventBus对象的创建</h3><p>通过EventBus.register(Object object)方法来注册订阅者（subscriber ），使用EventBus.post(Object event)方法来发布事件。 </p>\n<pre><code>    @Test\n    public void test1() {\n        //创建EventBus对象，并给予identifier\n        //identifier：此总线的简短名称，用于日志记录。应该是一个有效的java标识符。\n        EventBus eventBus = new EventBus(&quot;choxsu&quot;);\n\n        //注册所有的订阅\n        eventBus.register(new HelloEventListener());\n        //eventBus.register(new Hello2EventListener());\n\n        //发布事件\n        eventBus.post(new OrderEvent(&quot;hello&quot;));\n        eventBus.post(new OrderEvent(&quot;world&quot;));\n\n        eventBus.post(&quot;hello world&quot;);\n        eventBus.post(&quot;hello world2&quot;);\n\n        eventBus.post(1);\n        eventBus.post(12);\n    }\n</code></pre><h3 id=\"h3--listener-\"><a name=\"(订阅)Listener类的定义，具体实现见代码\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>(订阅)Listener类的定义，具体实现见代码</h3><p>使用Guava之后发布-订阅模式就变得很简单了，如果你需要订阅某种类型的消息，只需要在指定的方法上加上<a href=\"https://github.com/Subscribe\" title=\"&#64;Subscribe\" class=\"at-link\">@Subscribe</a>注解即可。</p>\n<h6 id=\"h6-1-subscriber-guava-subscriber-\"><a name=\"1.一个subscriber也可以同时订阅多个事件，Guava会通过事件类型来和订阅方法的形参来决定到底调用subscriber的哪个订阅方法\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>1.一个subscriber也可以同时订阅多个事件，Guava会通过事件类型来和订阅方法的形参来决定到底调用subscriber的哪个订阅方法</h6><p>EventBus post OrderEvent类型事件时，Hello2EventListener 的listen(OrderEvent event)方法会被调用，当post String类型事件时，Hello2EventListener 的listen(String event) 会被调用。</p>\n<pre><code>    class HelloEventListener {\n\n        @Subscribe\n        public void listen(OrderEvent event) {\n            System.out.println(&quot;receive1 msg:&quot; + event.getMessage());\n        }\n\n\n        @Subscribe\n        public void listen(String event) {\n            System.out.println(&quot;receive2 msg:&quot; + event);\n        }\n\n\n        @Subscribe\n        public void listen(Integer event) {\n            System.out.println(&quot;receive3 msg:&quot; + event);\n        }\n\n    }\n</code></pre><h6 id=\"h6-2-subscriber-subscriber-\"><a name=\"2.如果多个subscriber订阅了同一个事件，那么每个subscriber都将收到事件通知，并且收到事件通知的顺序跟注册的顺序保持一致\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>2.如果多个subscriber订阅了同一个事件，那么每个subscriber都将收到事件通知，并且收到事件通知的顺序跟注册的顺序保持一致</h6><pre><code>    class Hello2EventListener {\n        @Subscribe\n        public void listen(OrderEvent event) {\n            System.out.println(&quot;hello2 receive2 msg:&quot; + event.getMessage());\n        }\n        @Subscribe\n        public void listen(String event) {\n            System.out.println(&quot;hello2 receive2 msg:&quot; + event);\n        }\n    }\n</code></pre><pre><code>//注册所有的订阅\neventBus.register(new HelloEventListener());\neventBus.register(new Hello2EventListener());\n</code></pre><p>HelloEventListener和MultiEventListener都订阅了OrderEvent 事件，所以他们都会收到OrderEvent 事件通知。但是HelloEventListener会第一个收到OrderEvent 事件通知，其次是Hello2EventListener。</p>\n<h3 id=\"h3--event-pojo-\"><a name=\"自定义Event对象（一个简单POJO）\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>自定义Event对象（一个简单POJO）</h3><p>Guava 发布-订阅模式 中传递的事件，是一个普通的POJO类。</p>\n<pre><code>    class OrderEvent {\n        private String message;\n\n        public OrderEvent(String message) {\n            this.message = message;\n        }\n\n        public String getMessage() {\n            return message;\n        }\n    }\n</code></pre><h3 id=\"h3-u8F93u51FAu7ED3u679C\"><a name=\"输出结果\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>输出结果</h3><pre><code>receive1 msg:hello\nhello2 receive2 msg:hello\nreceive1 msg:world\nhello2 receive2 msg:world\nreceive2 msg:hello world\nhello2 receive2 msg:hello world\nreceive2 msg:hello world2\nhello2 receive2 msg:hello world2\nreceive3 msg:1\nreceive3 msg:12\n</code></pre>', ' **EventBus**允许在组件之间进行发布-订阅式的通信，而不需要组件之间显式地进行注册(从而相互了解)。它的设计完全是为了用显式注册代替传统的Java进程内事件分布。它不是通用的发布-订阅系统，也不是用于进程间通信的。\n###EventBus对象的创建\n通过EventBus.register(Object object)方法来注册订阅者（subscriber ），使用EventBus.post(Object event)方法来发布事件。 \n```\n    @Test\n    public void test1() {\n        //创建EventBus对象，并给予identifier\n        //identifier：此总线的简短名称，用于日志记录。应该是一个有效的java标识符。\n        EventBus eventBus = new EventBus(\"choxsu\");\n\n        //注册所有的订阅\n        eventBus.register(new HelloEventListener());\n        //eventBus.register(new Hello2EventListener());\n\n        //发布事件\n        eventBus.post(new OrderEvent(\"hello\"));\n        eventBus.post(new OrderEvent(\"world\"));\n\n        eventBus.post(\"hello world\");\n        eventBus.post(\"hello world2\");\n\n        eventBus.post(1);\n        eventBus.post(12);\n    }\n```\n###(订阅)Listener类的定义，具体实现见代码\n使用Guava之后发布-订阅模式就变得很简单了，如果你需要订阅某种类型的消息，只需要在指定的方法上加上@Subscribe注解即可。\n######1.一个subscriber也可以同时订阅多个事件，Guava会通过事件类型来和订阅方法的形参来决定到底调用subscriber的哪个订阅方法\nEventBus post OrderEvent类型事件时，Hello2EventListener 的listen(OrderEvent event)方法会被调用，当post String类型事件时，Hello2EventListener 的listen(String event) 会被调用。\n```\n    class HelloEventListener {\n\n        @Subscribe\n        public void listen(OrderEvent event) {\n            System.out.println(\"receive1 msg:\" + event.getMessage());\n        }\n\n\n        @Subscribe\n        public void listen(String event) {\n            System.out.println(\"receive2 msg:\" + event);\n        }\n\n\n        @Subscribe\n        public void listen(Integer event) {\n            System.out.println(\"receive3 msg:\" + event);\n        }\n\n    }\n```\n######2.如果多个subscriber订阅了同一个事件，那么每个subscriber都将收到事件通知，并且收到事件通知的顺序跟注册的顺序保持一致\n```\n    class Hello2EventListener {\n        @Subscribe\n        public void listen(OrderEvent event) {\n            System.out.println(\"hello2 receive2 msg:\" + event.getMessage());\n        }\n        @Subscribe\n        public void listen(String event) {\n            System.out.println(\"hello2 receive2 msg:\" + event);\n        }\n    }\n```\n\n```\n//注册所有的订阅\neventBus.register(new HelloEventListener());\neventBus.register(new Hello2EventListener());\n```\nHelloEventListener和MultiEventListener都订阅了OrderEvent 事件，所以他们都会收到OrderEvent 事件通知。但是HelloEventListener会第一个收到OrderEvent 事件通知，其次是Hello2EventListener。\n\n###自定义Event对象（一个简单POJO）\nGuava 发布-订阅模式 中传递的事件，是一个普通的POJO类。\n```\n    class OrderEvent {\n        private String message;\n\n        public OrderEvent(String message) {\n            this.message = message;\n        }\n\n        public String getMessage() {\n            return message;\n        }\n    }\n```\n\n###输出结果\n```\nreceive1 msg:hello\nhello2 receive2 msg:hello\nreceive1 msg:world\nhello2 receive2 msg:world\nreceive2 msg:hello world\nhello2 receive2 msg:hello world\nreceive2 msg:hello world2\nhello2 receive2 msg:hello world2\nreceive3 msg:1\nreceive3 msg:12\n```\n\n\n\n', '2018-06-14 17:18:09', '2018-06-14 17:18:09', 31, 0, 0, 'code', 0, 1, NULL);
INSERT INTO `blog` VALUES (32, 1, 'Linux下使用ps命令查看某个进程文件的启动位置', '<p> 使用ps命令，使用方法如下：</p>\n<p><code>ps -ef|grep shutdown</code></p>\n<p>其中shutdown为关机命令，但是此时查看到的只是相对路径，没有绝对路径，如：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-eab04570c3675dd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>其中4170就是进程ID，此时进入【/proc/4170】，并通过 ls -al查看如下：</p>\n<p><code>ls -al /proc/4170</code></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/7463793-c0abcd2f4ba45f6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>注意：</p>\n<ul>\n<li>cwd符号链接的是进程运行目录；</li><li>exe符号连接就是执行程序的绝对路径；</li><li>cmdline就是程序运行时输入的命令行命令；</li><li>environ记录了进程运行时的环境变量；</li><li>fd目录下是进程打开或使用的文件的符号连接。</li></ul>\n', ' 使用ps命令，使用方法如下：\n\n`ps -ef|grep shutdown`\n\n其中shutdown为关机命令，但是此时查看到的只是相对路径，没有绝对路径，如：\n\n![image](http://upload-images.jianshu.io/upload_images/7463793-eab04570c3675dd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n其中4170就是进程ID，此时进入【/proc/4170】，并通过 ls -al查看如下：\n\n`ls -al /proc/4170`\n\n![image](http://upload-images.jianshu.io/upload_images/7463793-c0abcd2f4ba45f6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n注意：\n\n*   cwd符号链接的是进程运行目录；\n*   exe符号连接就是执行程序的绝对路径；\n*   cmdline就是程序运行时输入的命令行命令；\n*   environ记录了进程运行时的环境变量；\n*   fd目录下是进程打开或使用的文件的符号连接。\n', '2018-06-14 17:18:43', '2018-06-14 17:18:43', 14, 0, 0, 'blog', 0, 16, NULL);
INSERT INTO `blog` VALUES (33, 1, 'Linux系统下的find文件查找', '<h3 id=\"h3-u76F4u63A5u8FDBu5165u6B63u9898\"><a name=\"直接进入正题\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>直接进入正题</h3><p>查找命令: <code>find</code></p>\n<h4 id=\"h4-u8BEDu6CD5\"><a name=\"语法\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>语法</h4><p><code>find(选项)(参数)</code></p>\n<h4 id=\"h4-u9009u9879\"><a name=\"选项\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>选项</h4><pre><code>-amin&lt;分钟&gt;：查找在指定时间曾被存取过的文件或目录，单位以分钟计算；\n\n-anewer&lt;参考文件或目录&gt;：查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；\n\n-atime&lt;24小时数&gt;：查找在指定时间曾被存取过的文件或目录，单位以24小时计算；\n\n-cmin&lt;分钟&gt;：查找在指定时间之时被更改过的文件或目录；\n\n-cnewer&lt;参考文件或目录&gt;查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n\n-ctime&lt;24小时数&gt;：查找在指定时间之时被更改的文件或目录，单位以24小时计算；\n\n-daystart：从本日开始计算时间；\n\n-depth：从指定目录下最深层的子目录开始查找；\n\n-expty：寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录；\n\n-[exec](http://man.linuxde.net/exec &quot;exec命令&quot;)&lt;执行指令&gt;：假设find指令的回传值为True，就执行该指令；\n\n-false：将find指令的回传值皆设为False；\n\n-fls&lt;列表文件&gt;：此参数的效果和指定“-[ls](http://man.linuxde.net/ls &quot;ls命令&quot;)”参数类似，但会把结果保存为指定的列表文件；\n\n-follow：排除符号连接；\n\n-fprint&lt;列表文件&gt;：此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；\n\n-fprint0&lt;列表文件&gt;：此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；\n\n-fprintf&lt;列表文件&gt;&lt;输出格式&gt;：此参数的效果和指定“-[printf](http://man.linuxde.net/printf &quot;printf命令&quot;)”参数类似，但会把结果保存成指定的列表文件；\n\n-fstype&lt;文件系统类型&gt;：只寻找该文件系统类型下的文件或目录；\n\n-gid&lt;群组识别码&gt;：查找符合指定之群组识别码的文件或目录；\n\n-group&lt;群组名称&gt;：查找符合指定之群组名称的文件或目录；\n\n-[help](http://man.linuxde.net/help &quot;help命令&quot;)或——help：在线帮助；\n\n-ilname&lt;范本样式&gt;：此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；\n\n-iname&lt;范本样式&gt;：此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；\n\n-inum&lt;inode编号&gt;：查找符合指定的inode编号的文件或目录；\n\n-ipath&lt;范本样式&gt;：此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；\n\n-iregex&lt;范本样式&gt;：此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；\n\n-links&lt;连接数目&gt;：查找符合指定的硬连接数目的文件或目录；\n\n-iname&lt;范本样式&gt;：指定字符串作为寻找符号连接的范本样式；\n\n-ls：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出；\n\n-maxdepth&lt;目录层级&gt;：设置最大目录层级；\n\n-mindepth&lt;目录层级&gt;：设置最小目录层级；\n\n-mmin&lt;分钟&gt;：查找在指定时间曾被更改过的文件或目录，单位以分钟计算；\n\n-[mount](http://man.linuxde.net/mount &quot;mount命令&quot;)：此参数的效果和指定“-xdev”相同；\n\n-mtime&lt;24小时数&gt;：查找在指定时间曾被更改过的文件或目录，单位以24小时计算；\n\n-name&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；\n\n-newer&lt;参考文件或目录&gt;：查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n\n-nogroup：找出不属于本地主机群组识别码的文件或目录；\n\n-noleaf：不去考虑目录至少需拥有两个硬连接存在；\n\n-nouser：找出不属于本地主机用户识别码的文件或目录；\n\n-ok&lt;执行指令&gt;：此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放\n弃执行命令；\n\n-path&lt;范本样式&gt;：指定字符串作为寻找目录的范本样式；\n\n-perm&lt;权限数值&gt;：查找符合指定的权限数值的文件或目录；\n\n-print：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“./”字符串；\n\n-print0：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；\n\n-printf&lt;输出格式&gt;：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；\n\n-prune：不寻找字符串作为寻找文件或目录的范本样式;\n\n-regex&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；\n\n-size&lt;文件大小&gt;：查找符合指定的文件大小的文件；\n\n-true：将find指令的回传值皆设为True；\n\n-typ&lt;文件类型&gt;：只寻找符合指定的文件类型的文件；\n\n-uid&lt;用户识别码&gt;：查找符合指定的用户识别码的文件或目录；\n\n-used&lt;日数&gt;：查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；\n\n-user&lt;拥有者名称&gt;：查找符和指定的拥有者名称的文件或目录；\n\n-version或——version：显示版本信息；\n\n-xdev：将范围局限在先行的文件系统中；\n\n-xtype&lt;文件类型&gt;：此参数的效果和指定“-[type](http://man.linuxde.net/type &quot;type命令&quot;)”参数类似，差别在于它针对符号连接检查。&lt;/pre&gt;\n</code></pre><h4 id=\"h4-u53C2u6570\"><a name=\"参数\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>参数</h4><p>起始目录：查找文件的起始目录。</p>\n<h4 id=\"h4-u5B9Eu4F8B\"><a name=\"实例\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>实例</h4><p><code>根据文件或者正则表达式进行匹配</code></p>\n<p>列出当前目录及子目录下所有文件和文件夹<br><code>find .</code></p>\n<p>在<code>/home</code>目录下查找以.txt结尾的文件名</p>\n<p><code>find /home -name &quot;*.txt&quot;</code></p>\n<p>同上，但忽略大小写</p>\n<p><code>find /home -iname &quot;*.txt&quot;</code></p>\n<p>当前目录及子目录下查找所有以.txt和.pdf结尾的文件</p>\n<p><code>find . \\( -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; \\)</code>或<code>find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot;</code></p>\n<p>匹配文件路径或者文件</p>\n<p><code>find /usr/ -path &quot;*local*&quot;</code></p>\n<p>基于正则表达式匹配文件路径</p>\n<p><code>find . -regex &quot;.*\\(\\.txt\\|\\.pdf\\)$&quot;</code></p>\n<p>同上，但忽略大小写</p>\n<p><code>find . -iregex &quot;.*\\(\\.txt\\|\\.pdf\\)$&quot;</code></p>\n<h4 id=\"h4-u5426u5B9Au53C2u6570\"><a name=\"否定参数\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>否定参数</h4><p>找出/home下不是以.txt结尾的文件</p>\n<p><code>find /home ! -name &quot;*.txt&quot;</code></p>\n<h4 id=\"h4-u6839u636Eu6587u4EF6u7C7Bu578Bu8FDBu884Cu641Cu7D22\"><a name=\"根据文件类型进行搜索\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>根据文件类型进行搜索</h4><p><code>find . -type 类型参数</code></p>\n<p>类型参数列表：</p>\n<ul>\n<li><strong>f </strong>普通文件</li><li><strong>l </strong>符号连接</li><li><strong>d</strong> 目录</li><li><strong>c </strong>字符设备</li><li><strong>b </strong>块设备</li><li><strong>s </strong>套接字</li><li><strong>p </strong>Fifo</li></ul>\n<h4 id=\"h4-u57FAu4E8Eu76EEu5F55u6DF1u5EA6u641Cu7D22\"><a name=\"基于目录深度搜索\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>基于目录深度搜索</h4><p>向下最大深度限制为3</p>\n<p><code>find . -maxdepth 3 -type f</code></p>\n<p>搜索出深度距离当前目录至少2个子目录的所有文件</p>\n<p><code>find . -mindepth 2 -type f</code></p>\n<h4 id=\"h4-u6839u636Eu6587u4EF6u65F6u95F4u6233u8FDBu884Cu641Cu7D22\"><a name=\"根据文件时间戳进行搜索\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>根据文件时间戳进行搜索</h4><p><code>find . -type f 时间戳</code></p>\n<p>UNIX/Linux文件系统每个文件都有三种时间戳：</p>\n<ul>\n<li><strong>访问时间</strong>（-atime/天，-amin/分钟）：用户最近一次访问时间。</li><li><strong>修改时间</strong>（-mtime/天，-mmin/分钟）：文件最后一次修改时间。</li><li><strong>变化时间</strong>（-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。</li></ul>\n<p>搜索最近七天内被访问过的所有文件<br><code>find . -type f -atime -7</code></p>\n<p>搜索恰好在七天前被访问过的所有文件</p>\n<p><code>find . -type f -atime 7</code></p>\n<p>搜索超过七天内被访问过的所有文件</p>\n<p><code>find . -type f -atime +7</code></p>\n<p>搜索访问时间超过10分钟的所有文件</p>\n<p><code>find . -type f -amin +10</code></p>\n<p>找出比<a href=\"http://man.linuxde.net/file\" title=\"file命令\">file</a>.log修改时间更长的所有文件<br><code>find . -type f -newer file.log</code></p>\n<h4 id=\"h4-u6839u636Eu6587u4EF6u5927u5C0Fu8FDBu884Cu5339u914D\"><a name=\"根据文件大小进行匹配\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>根据文件大小进行匹配</h4><p><code>find . -type f -size 文件大小单元</code></p>\n<p>文件大小单元：</p>\n<ul>\n<li><strong>b</strong> —— 块（512字节）</li><li><strong>c</strong> —— 字节</li><li><strong><a href=\"http://man.linuxde.net/w\" title=\"w命令\">w</a></strong> —— 字（2字节）</li><li><strong>k</strong> —— 千字节</li><li><strong>M</strong> —— 兆字节</li><li><strong>G</strong> —— 吉字节</li></ul>\n<p>搜索大于10KB的文件</p>\n<p><code>find . -type f -size +10k</code></p>\n<p>搜索小于10KB的文件</p>\n<p><code>find . -type f -size -10k</code></p>\n<p>搜索等于10KB的文件</p>\n<p><code>find . -type f -size 10k</code></p>\n<h4 id=\"h4-u5220u9664u5339u914Du6587u4EF6\"><a name=\"删除匹配文件\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>删除匹配文件</h4><p>删除当前目录下所有.txt文件</p>\n<p><code>find . -type f -name &quot;*.txt&quot; -delete</code></p>\n<h4 id=\"h4--\"><a name=\"根据文件权限/所有权进行匹配\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>根据文件权限/所有权进行匹配</h4><p>当前目录下搜索出权限为777的文件<br><code>find . -type f -perm 777</code></p>\n<p>找出当前目录下权限不是644的<a href=\"http://man.linuxde.net/php\" title=\"php命令\">php</a>文件</p>\n<p><code>find . -type f -name &quot;*.php&quot; ! -perm 644</code></p>\n<p>找出当前目录用户tom拥有的所有文件</p>\n<p><code>find . -type f -user tom</code></p>\n<p>找出当前目录用户组sunk拥有的所有文件</p>\n<p><code>find . -type f -group sunk</code></p>\n<h4 id=\"h4--code-exec-code-\"><a name=\"借助<code>-exec</code>选项与其他命令结合使用\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>借助<code>-exec</code>选项与其他命令结合使用</h4><p>找出当前目录下所有root的文件，并把所有权更改为用户tom</p>\n<p><code>find .-type f -user root -exec [chown](http://man.linuxde.net/chown &quot;chown命令&quot;) tom {} \\;</code></p>\n<p>上例中，<strong>{}</strong> 用于与<strong>-exec</strong>选项结合使用来匹配所有文件，然后会被替换为相应的文件名。</p>\n<p>找出自己家目录下所有的.txt文件并删除</p>\n<p><code>find $HOME/. -name &quot;*.txt&quot; -ok [rm](http://man.linuxde.net/rm &quot;rm命令&quot;) {} \\;</code></p>\n<p>上例中，<strong>-ok</strong>和<strong>-exec</strong>行为一样，不过它会给出提示，是否执行相应的操作。</p>\n<p>查找当前目录下所有.txt文件并把他们拼接起来写入到all.txt文件中</p>\n<p><code>find . -type f -name &quot;*.txt&quot; -exec [cat](http://man.linuxde.net/cat &quot;cat命令&quot;) {} \\;&gt; all.txt</code></p>\n<p>将30天前的.log文件移动到old目录中</p>\n<p><code>find . -type f -mtime +30 -name &quot;*.log&quot; -exec [cp](http://man.linuxde.net/cp &quot;cp命令&quot;) {} old \\;</code></p>\n<p>找出当前目录下所有.txt文件并以“File:文件名”的形式打印出来</p>\n<p><code>find . -type f -name &quot;*.txt&quot; -exec printf &quot;File: %s\\n&quot; {} \\;</code></p>\n<p>因为单行命令中-exec参数中无法使用多个命令，以下方法可以实现在-exec之后接受多条命令</p>\n<p><code>-exec ./text.sh {} \\;</code></p>\n<h4 id=\"h4-u641Cu7D22u4F46u8DF3u51FAu6307u5B9Au7684u76EEu5F55\"><a name=\"搜索但跳出指定的目录\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>搜索但跳出指定的目录</h4><p>查找当前目录或者子目录下所有.txt文件，但是跳过子目录sk</p>\n<p><code>find . -path &quot;./sk&quot; -prune -o -name &quot;*.txt&quot; -print</code></p>\n<h4 id=\"h4-find-\"><a name=\"find其他技巧收集\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>find其他技巧收集</h4><p>要列出所有长度为零的文件</p>\n<p><code>find . -empty</code></p>\n', ' ### 直接进入正题\n查找命令: `find`\n#### 语法\n`\nfind(选项)(参数)\n`\n\n#### 选项\n```\n-amin<分钟>：查找在指定时间曾被存取过的文件或目录，单位以分钟计算；\n\n-anewer<参考文件或目录>：查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；\n\n-atime<24小时数>：查找在指定时间曾被存取过的文件或目录，单位以24小时计算；\n\n-cmin<分钟>：查找在指定时间之时被更改过的文件或目录；\n\n-cnewer<参考文件或目录>查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n\n-ctime<24小时数>：查找在指定时间之时被更改的文件或目录，单位以24小时计算；\n\n-daystart：从本日开始计算时间；\n\n-depth：从指定目录下最深层的子目录开始查找；\n\n-expty：寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录；\n\n-[exec](http://man.linuxde.net/exec \"exec命令\")<执行指令>：假设find指令的回传值为True，就执行该指令；\n\n-false：将find指令的回传值皆设为False；\n\n-fls<列表文件>：此参数的效果和指定“-[ls](http://man.linuxde.net/ls \"ls命令\")”参数类似，但会把结果保存为指定的列表文件；\n\n-follow：排除符号连接；\n\n-fprint<列表文件>：此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；\n\n-fprint0<列表文件>：此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；\n\n-fprintf<列表文件><输出格式>：此参数的效果和指定“-[printf](http://man.linuxde.net/printf \"printf命令\")”参数类似，但会把结果保存成指定的列表文件；\n\n-fstype<文件系统类型>：只寻找该文件系统类型下的文件或目录；\n\n-gid<群组识别码>：查找符合指定之群组识别码的文件或目录；\n\n-group<群组名称>：查找符合指定之群组名称的文件或目录；\n\n-[help](http://man.linuxde.net/help \"help命令\")或——help：在线帮助；\n\n-ilname<范本样式>：此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；\n\n-iname<范本样式>：此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；\n\n-inum<inode编号>：查找符合指定的inode编号的文件或目录；\n\n-ipath<范本样式>：此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；\n\n-iregex<范本样式>：此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；\n\n-links<连接数目>：查找符合指定的硬连接数目的文件或目录；\n\n-iname<范本样式>：指定字符串作为寻找符号连接的范本样式；\n\n-ls：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出；\n\n-maxdepth<目录层级>：设置最大目录层级；\n\n-mindepth<目录层级>：设置最小目录层级；\n\n-mmin<分钟>：查找在指定时间曾被更改过的文件或目录，单位以分钟计算；\n\n-[mount](http://man.linuxde.net/mount \"mount命令\")：此参数的效果和指定“-xdev”相同；\n\n-mtime<24小时数>：查找在指定时间曾被更改过的文件或目录，单位以24小时计算；\n\n-name<范本样式>：指定字符串作为寻找文件或目录的范本样式；\n\n-newer<参考文件或目录>：查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n\n-nogroup：找出不属于本地主机群组识别码的文件或目录；\n\n-noleaf：不去考虑目录至少需拥有两个硬连接存在；\n\n-nouser：找出不属于本地主机用户识别码的文件或目录；\n\n-ok<执行指令>：此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放\n弃执行命令；\n\n-path<范本样式>：指定字符串作为寻找目录的范本样式；\n\n-perm<权限数值>：查找符合指定的权限数值的文件或目录；\n\n-print：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“./”字符串；\n\n-print0：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；\n\n-printf<输出格式>：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；\n\n-prune：不寻找字符串作为寻找文件或目录的范本样式;\n\n-regex<范本样式>：指定字符串作为寻找文件或目录的范本样式；\n\n-size<文件大小>：查找符合指定的文件大小的文件；\n\n-true：将find指令的回传值皆设为True；\n\n-typ<文件类型>：只寻找符合指定的文件类型的文件；\n\n-uid<用户识别码>：查找符合指定的用户识别码的文件或目录；\n\n-used<日数>：查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；\n\n-user<拥有者名称>：查找符和指定的拥有者名称的文件或目录；\n\n-version或——version：显示版本信息；\n\n-xdev：将范围局限在先行的文件系统中；\n\n-xtype<文件类型>：此参数的效果和指定“-[type](http://man.linuxde.net/type \"type命令\")”参数类似，差别在于它针对符号连接检查。</pre>\n```\n#### 参数\n\n起始目录：查找文件的起始目录。\n\n#### 实例\n``根据文件或者正则表达式进行匹配``\n\n列出当前目录及子目录下所有文件和文件夹\n`find .`\n\n在`/home`目录下查找以.txt结尾的文件名\n\n`find /home -name \"*.txt\"`\n\n同上，但忽略大小写\n\n`find /home -iname \"*.txt\"`\n\n当前目录及子目录下查找所有以.txt和.pdf结尾的文件\n\n`find . \\( -name \"*.txt\" -o -name \"*.pdf\" \\)`或`find . -name \"*.txt\" -o -name \"*.pdf\" `\n\n匹配文件路径或者文件\n\n`find /usr/ -path \"*local*\"`\n\n基于正则表达式匹配文件路径\n\n`find . -regex \".*\\(\\.txt\\|\\.pdf\\)$\"`\n\n同上，但忽略大小写\n\n`find . -iregex \".*\\(\\.txt\\|\\.pdf\\)$\"`\n\n#### 否定参数\n\n找出/home下不是以.txt结尾的文件\n\n`find /home ! -name \"*.txt\"`\n\n#### 根据文件类型进行搜索\n\n`find . -type 类型参数`\n\n类型参数列表：\n\n*   **f **普通文件\n*   **l **符号连接\n*   **d** 目录\n*   **c **字符设备\n*   **b **块设备\n*   **s **套接字\n*   **p **Fifo\n\n#### 基于目录深度搜索\n\n向下最大深度限制为3\n\n`find . -maxdepth 3 -type f`\n\n搜索出深度距离当前目录至少2个子目录的所有文件\n\n`find . -mindepth 2 -type f`\n\n#### 根据文件时间戳进行搜索\n`find . -type f 时间戳`\n\nUNIX/Linux文件系统每个文件都有三种时间戳：\n\n*   **访问时间**（-atime/天，-amin/分钟）：用户最近一次访问时间。\n*   **修改时间**（-mtime/天，-mmin/分钟）：文件最后一次修改时间。\n*   **变化时间**（-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。\n\n搜索最近七天内被访问过的所有文件\n`find . -type f -atime -7`\n\n搜索恰好在七天前被访问过的所有文件\n\n`find . -type f -atime 7`\n\n搜索超过七天内被访问过的所有文件\n\n`find . -type f -atime +7`\n\n搜索访问时间超过10分钟的所有文件\n\n`find . -type f -amin +10`\n\n找出比[file](http://man.linuxde.net/file \"file命令\").log修改时间更长的所有文件\n`find . -type f -newer file.log`\n\n#### 根据文件大小进行匹配\n`find . -type f -size 文件大小单元`\n\n文件大小单元：\n\n*   **b** —— 块（512字节）\n*   **c** —— 字节\n*   **[w](http://man.linuxde.net/w \"w命令\")** —— 字（2字节）\n*   **k** —— 千字节\n*   **M** —— 兆字节\n*   **G** —— 吉字节\n\n搜索大于10KB的文件\n\n`find . -type f -size +10k`\n\n搜索小于10KB的文件\n\n`find . -type f -size -10k`\n\n搜索等于10KB的文件\n\n`find . -type f -size 10k`\n\n#### 删除匹配文件\n\n删除当前目录下所有.txt文件\n\n`find . -type f -name \"*.txt\" -delete`\n\n#### 根据文件权限/所有权进行匹配\n\n当前目录下搜索出权限为777的文件\n`find . -type f -perm 777`\n\n找出当前目录下权限不是644的[php](http://man.linuxde.net/php \"php命令\")文件\n\n`find . -type f -name \"*.php\" ! -perm 644`\n\n找出当前目录用户tom拥有的所有文件\n\n`find . -type f -user tom`\n\n找出当前目录用户组sunk拥有的所有文件\n\n`find . -type f -group sunk`\n\n#### 借助`-exec`选项与其他命令结合使用\n\n找出当前目录下所有root的文件，并把所有权更改为用户tom\n\n`find .-type f -user root -exec [chown](http://man.linuxde.net/chown \"chown命令\") tom {} \\;`\n\n上例中，**{}** 用于与**-exec**选项结合使用来匹配所有文件，然后会被替换为相应的文件名。\n\n找出自己家目录下所有的.txt文件并删除\n\n`find $HOME/. -name \"*.txt\" -ok [rm](http://man.linuxde.net/rm \"rm命令\") {} \\;`\n\n上例中，**-ok**和**-exec**行为一样，不过它会给出提示，是否执行相应的操作。\n\n查找当前目录下所有.txt文件并把他们拼接起来写入到all.txt文件中\n\n`find . -type f -name \"*.txt\" -exec [cat](http://man.linuxde.net/cat \"cat命令\") {} \\;> all.txt`\n\n将30天前的.log文件移动到old目录中\n\n`find . -type f -mtime +30 -name \"*.log\" -exec [cp](http://man.linuxde.net/cp \"cp命令\") {} old \\;`\n\n找出当前目录下所有.txt文件并以“File:文件名”的形式打印出来\n\n`find . -type f -name \"*.txt\" -exec printf \"File: %s\\n\" {} \\;`\n\n因为单行命令中-exec参数中无法使用多个命令，以下方法可以实现在-exec之后接受多条命令\n\n`-exec ./text.sh {} \\;`\n\n#### 搜索但跳出指定的目录\n\n查找当前目录或者子目录下所有.txt文件，但是跳过子目录sk\n\n`find . -path \"./sk\" -prune -o -name \"*.txt\" -print`\n\n#### find其他技巧收集\n\n要列出所有长度为零的文件\n\n`find . -empty`\n\n', '2018-06-14 17:19:03', '2018-06-14 17:19:03', 16, 0, 0, 'blog', 0, 16, NULL);
INSERT INTO `blog` VALUES (34, 1, 'Nginx、Tomcat 开启Gzip，提升网站静态资源打开速度', '<h3 id=\"h3-nginx-\"><a name=\"Nginx配置\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>Nginx配置</h3><p>进入正题：环境。阿里云服务器 centos7<br><code>vim  /etc/nginx/nginx.conf</code><br>增加配置</p>\n<pre><code>gzip on;\ngzip_min_length  1k;\ngzip_buffers 4 16k;\n#gzip_http_version 1.0;\ngzip_comp_level 2;\ngzip_types text/plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;\ngzip_vary off;\ngzip_disable &quot;MSIE [1-6]\\.&quot;;\n</code></pre><h3 id=\"h3-u914Du7F6Eu89E3u91CA\"><a name=\"配置解释\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>配置解释</h3><pre><code>gzip on：开启Gzip\ngzip_min_length：不压缩临界值，大于1K的才压缩，一般不用改\ngzip_buffers：buffer，默认值: gzip_buffers 4 4k/8k\ngzip_http_version：用了反向代理的话，末端通信是HTTP/1.0，有需求的应该也不用看我这科普文了；有这句的话注释了就行了，默认是HTTP/1.1\ngzip_comp_level：压缩级别，1-10，数字越大压缩的越好，时间也越长，但是CPU压力越大，综合考虑吧\ngzip_types：压缩类型，匹配MIME类型进行压缩 # 不能用通配符 text/* # (无论是否指定)text/html默认已经压缩 # 设置哪压缩种文本文件可参考 conf/mime.types\ngzip_vary： 和http头有关系，加个vary头，给代理服务器用的，有的浏览器支持压缩，有的不支持，所以避免浪费不支持的也压缩，所以根据客户端的HTTP头来判断，是否需要压缩\ngzip_disable：IE6对Gzip不怎么友好，不给它Gzip；禁用IE6的gzip压缩，又是因为杯具的IE6。当然，IE6目前依然广泛的存在，所以这里你也可以设置为“MSIE [1-5].\n</code></pre><h3 id=\"h3--nginx-conf-\"><a name=\"检查nginx.conf文件\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>检查nginx.conf文件</h3><p><code>cd /sbin</code><br><code>./nginx -t</code><br>出现</p>\n<pre><code class=\"lang-`\">inx: the configuration file /etc/nginx/nginx.conf syntax is ok \nginx: configuration file /etc/nginx/nginx.conf test is successful\n</code></pre>\n<p>表示Nginx文件配置没有问题<br>然后就是重启Nginx了<br><code>/nginx -s reload</code></p>\n<h3 id=\"h3-tomcat-\"><a name=\"Tomcat配置\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>Tomcat配置</h3><p>编辑tomcat配置文件<br><code>vim /data/java/apache-tomcat-1013/conf/server.xml</code></p>\n<pre><code>&lt;Connerctor port=&quot;80&quot; protocol=&quot;HTTP/1.1&quot;  connectionTimeout=&quot;20000&quot;\nredirectPort=&quot;8443&quot; executor=&quot;tomcatThreadPool&quot; URIEncoding=&quot;utf-8&quot;   \ncompression=&quot;on&quot;   \ncompressionMinSize=&quot;50&quot; noCompressionUserAgents=&quot;gozilla, traviata&quot;   \ncompressableMimeType=&quot;text/html,text/xml,text/javascript,text/css,text/plain&quot; /&gt;\n</code></pre><p>重启Tomcat</p>\n<h3 id=\"h3-u6D4Bu8BD5\"><a name=\"测试\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>测试</h3><p>使用curl进行测试<br><code>curl -I -H &quot;Accept-Encoding: gzip, deflate&quot; &quot;http://127.0.0.1:1013&quot;</code></p>\n<pre><code>HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nContent-Type: text/html;charset=UTF-8\nTransfer-Encoding: chunked\nContent-Encoding: gzip\nVary: Accept-Encoding\nDate: Wed, 10 Jan 2018 08:25:55 GMT\n</code></pre><p>出现Content-Encoding: gzip这个就表示开启Gzip成功<br>使用浏览器测试</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/7463793-5692cbd4f1b12209.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>红色框表示开启Gzip成功</p>\n', '###  Nginx配置\n进入正题：环境。阿里云服务器 centos7\n`vim  /etc/nginx/nginx.conf`\n增加配置\n```\ngzip on;\ngzip_min_length  1k;\ngzip_buffers 4 16k;\n#gzip_http_version 1.0;\ngzip_comp_level 2;\ngzip_types text/plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;\ngzip_vary off;\ngzip_disable \"MSIE [1-6]\\.\";\n```\n### 配置解释\n```\ngzip on：开启Gzip\ngzip_min_length：不压缩临界值，大于1K的才压缩，一般不用改\ngzip_buffers：buffer，默认值: gzip_buffers 4 4k/8k\ngzip_http_version：用了反向代理的话，末端通信是HTTP/1.0，有需求的应该也不用看我这科普文了；有这句的话注释了就行了，默认是HTTP/1.1\ngzip_comp_level：压缩级别，1-10，数字越大压缩的越好，时间也越长，但是CPU压力越大，综合考虑吧\ngzip_types：压缩类型，匹配MIME类型进行压缩 # 不能用通配符 text/* # (无论是否指定)text/html默认已经压缩 # 设置哪压缩种文本文件可参考 conf/mime.types\ngzip_vary： 和http头有关系，加个vary头，给代理服务器用的，有的浏览器支持压缩，有的不支持，所以避免浪费不支持的也压缩，所以根据客户端的HTTP头来判断，是否需要压缩\ngzip_disable：IE6对Gzip不怎么友好，不给它Gzip；禁用IE6的gzip压缩，又是因为杯具的IE6。当然，IE6目前依然广泛的存在，所以这里你也可以设置为“MSIE [1-5].\n```\n\n### 检查nginx.conf文件\n`cd /sbin`\n`./nginx -t     `\n出现\n````\ninx: the configuration file /etc/nginx/nginx.conf syntax is ok \nginx: configuration file /etc/nginx/nginx.conf test is successful\n```\n表示Nginx文件配置没有问题\n然后就是重启Nginx了\n`/nginx -s reload`\n\n\n### Tomcat配置\n编辑tomcat配置文件\n`vim /data/java/apache-tomcat-1013/conf/server.xml`\n```\n<Connerctor port=\"80\" protocol=\"HTTP/1.1\"  connectionTimeout=\"20000\"\nredirectPort=\"8443\" executor=\"tomcatThreadPool\" URIEncoding=\"utf-8\"   \ncompression=\"on\"   \ncompressionMinSize=\"50\" noCompressionUserAgents=\"gozilla, traviata\"   \ncompressableMimeType=\"text/html,text/xml,text/javascript,text/css,text/plain\" />\n```\n重启Tomcat\n\n\n### 测试\n使用curl进行测试\n`curl -I -H \"Accept-Encoding: gzip, deflate\" \"http://127.0.0.1:1013\"`\n\n```\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nContent-Type: text/html;charset=UTF-8\nTransfer-Encoding: chunked\nContent-Encoding: gzip\nVary: Accept-Encoding\nDate: Wed, 10 Jan 2018 08:25:55 GMT\n```\n出现Content-Encoding: gzip这个就表示开启Gzip成功\n使用浏览器测试\n\n![image](https://upload-images.jianshu.io/upload_images/7463793-5692cbd4f1b12209.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n红色框表示开启Gzip成功\n ', '2018-06-26 09:19:51', '2018-06-14 17:23:46', 14, 0, 0, 'blog', 0, 16, NULL);
INSERT INTO `blog` VALUES (35, 1, 'linux下mysql数据的导出和导入', '<h4 id=\"h4-u5BFCu51FAu6574u4E2Au6570u636Eu5E93u4E2Du7684u6240u6709u6570u636E\"><a name=\"导出整个数据库中的所有数据\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导出整个数据库中的所有数据</h4><p>1、在linux命令行下输入：</p>\n<pre><code>mysqldump -u userName -p  dabaseName  &gt; fileName.sql\n</code></pre><p><code>fileName.sql最好加上路径名</code></p>\n<h4 id=\"h4-u5BFCu51FAu6570u636Eu5E93u4E2Du7684u67D0u4E2Au8868u7684u6570u636E\"><a name=\"导出数据库中的某个表的数据\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导出数据库中的某个表的数据</h4><pre><code>mysqldump -u userName -p  dabaseName tableName &gt; fileName.sql\n</code></pre><h4 id=\"h4-u5BFCu51FAu6574u4E2Au6570u636Eu5E93u4E2Du7684u6240u6709u7684u8868u7ED3u6784\"><a name=\"导出整个数据库中的所有的表结构\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导出整个数据库中的所有的表结构</h4><p>在linux命令行下输入：</p>\n<pre><code>mysqldump -u userName -p -d dabaseName  &gt; fileName.sql\n</code></pre><p><code>注意：是加了-d</code></p>\n<h4 id=\"h4-u5BFCu51FAu6574u4E2Au6570u636Eu5E93u4E2Du67D0u4E2Au8868u7684u8868u7ED3u6784\"><a name=\"导出整个数据库中某个表的表结构\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导出整个数据库中某个表的表结构</h4><p>在linux命令行下输入：</p>\n<pre><code>mysqldump -u userName -p -d dabaseName tableName &gt; fileName.sql\n</code></pre><p><code>注意：是加了-d</code></p>\n<h4 id=\"h4--mysql-1\"><a name=\"导入mysql方法1\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导入mysql方法1</h4><p>进入linux命令命令行下：</p>\n<pre><code>mysql -uroot -p 回车  输入密码\nsource fileName.sql\n注意fileName.sql要有路径名，例如：source /home/user/data/fileName.sql\n</code></pre><h4 id=\"h4--mysql-2\"><a name=\"导入mysql方法2\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>导入mysql方法2</h4><p>进入linux命令命令行下：</p>\n<pre><code>mysql -uroot -p database &lt; fileName.sql\n</code></pre><p><code>注意fileName.sql要有路径名</code></p>\n', ' #### 导出整个数据库中的所有数据\n\n1、在linux命令行下输入：\n\n```\nmysqldump -u userName -p  dabaseName  > fileName.sql \n```\n`fileName.sql最好加上路径名`\n\n #### 导出数据库中的某个表的数据\n```\nmysqldump -u userName -p  dabaseName tableName > fileName.sql \n```\n #### 导出整个数据库中的所有的表结构\n\n在linux命令行下输入：\n\n```\nmysqldump -u userName -p -d dabaseName  > fileName.sql\n```\n`注意：是加了-d `\n\n#### 导出整个数据库中某个表的表结构\n\n在linux命令行下输入：\n```\nmysqldump -u userName -p -d dabaseName tableName > fileName.sql\n```\n`注意：是加了-d`\n\n#### 导入mysql方法1\n\n进入linux命令命令行下：\n```\nmysql -uroot -p 回车  输入密码\nsource fileName.sql\n注意fileName.sql要有路径名，例如：source /home/user/data/fileName.sql\n```\n#### 导入mysql方法2\n\n进入linux命令命令行下：\n```\nmysql -uroot -p database < fileName.sql\n```\n`注意fileName.sql要有路径名`', '2018-07-05 22:10:51', '2018-07-05 22:10:51', 29, 0, 0, 'favorite', 0, 8, NULL);
INSERT INTO `blog` VALUES (36, 1, 'ssh远程登录命令简单实例', '<h4 id=\"h4-ssh-\"><a name=\"ssh远程登录命令简单实例\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>ssh远程登录命令简单实例</h4><p>ssh命令用于远程登录上Linux主机。</p>\n<p>常用格式：ssh [-l login_name] [-p port] [user@]hostname<br>更详细的可以用ssh -h查看。</p>\n<h4 id=\"h4-u4E3Eu4F8B\"><a name=\"举例\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>举例</h4><h6 id=\"h6--\"><a name=\"不指定用户：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>不指定用户：</h6><p><code>ssh 192.168.0.11</code></p>\n<h6 id=\"h6--\"><a name=\"指定用户：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>指定用户：</h6><p><code>ssh -l root 192.168.0.11</code></p>\n<p><code>ssh <a href=\"mailto:root@192.168.0\">root@192.168.0</a>.11</code></p>\n<h6 id=\"h6--ssh-\"><a name=\"如果修改过ssh登录端口的可以：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>如果修改过ssh登录端口的可以：</h6><p><code>ssh -p 12333 192.168.0.11</code></p>\n<p><code>ssh -l root -p 12333 216.230.230.114</code></p>\n<p><code>ssh -p 12333 <a href=\"mailto:root@216.230.230\">root@216.230.230</a>.114</code></p>\n<h6 id=\"h6--etc-ssh-sshd_config-ssh-root-\"><a name=\"另外修改配置文件/etc/ssh/sshd_config，可以改ssh登录端口和禁止root登录。改端口可以防止被端口扫描。\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>另外修改配置文件/etc/ssh/sshd_config，可以改ssh登录端口和禁止root登录。改端口可以防止被端口扫描。</h6><p>编辑配置文件：</p>\n<p><code>vim /etc/ssh/sshd_config</code></p>\n<p>找到#Port 22，去掉注释，修改成一个五位的端口：</p>\n<p><code>Port 12333</code></p>\n<p>找到#PermitRootLogin yes，去掉注释，修改为：</p>\n<p><code>PermitRootLogin no</code></p>\n<h6 id=\"h6--sshd-\"><a name=\"重启sshd服务：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>重启sshd服务：</h6><p><code>service sshd restart</code></p>\n', '####ssh远程登录命令简单实例\n \nssh命令用于远程登录上Linux主机。\n \n常用格式：ssh [-l login_name] [-p port] [user@]hostname\n更详细的可以用ssh -h查看。\n \n####举例\n \n######不指定用户：\n \n`ssh 192.168.0.11`\n \n######指定用户：\n \n`ssh -l root 192.168.0.11`\n \n`ssh root@192.168.0.11`\n \n######如果修改过ssh登录端口的可以：\n \n`ssh -p 12333 192.168.0.11`\n \n`ssh -l root -p 12333 216.230.230.114`\n \n`ssh -p 12333 root@216.230.230.114`\n \n######另外修改配置文件/etc/ssh/sshd_config，可以改ssh登录端口和禁止root登录。改端口可以防止被端口扫描。\n \n编辑配置文件：\n \n`vim /etc/ssh/sshd_config`\n \n找到#Port 22，去掉注释，修改成一个五位的端口：\n \n`Port 12333`\n \n找到#PermitRootLogin yes，去掉注释，修改为：\n \n`PermitRootLogin no`\n \n######重启sshd服务：\n \n`service sshd restart`', '2018-07-05 22:24:37', '2018-07-05 22:24:37', 17, 0, 0, 'favorite', 0, 16, NULL);
INSERT INTO `blog` VALUES (37, 1, 'JRbel激活服务', '<h2 id=\"h2-jrebel-idea-\"><a name=\"jrebel idea插件激活，亲测可用：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>jrebel idea插件激活，亲测可用：</h2><h4 id=\"h4--jrebel-server-\"><a name=\"在jrebel server处，写上：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>在jrebel server处，写上：</h4><p><a href=\"http://139.199.89.239:1008/88414687-3b91-4286-89ba-2dc813b107ce\">http://139.199.89.239:1008/88414687-3b91-4286-89ba-2dc813b107ce</a></p>\n<p>如果出现激活过期的情况，请重新生成guid，替换原来的guid即可</p>\n<p>邮箱随便写，即可激活。</p>\n', 'jrebel idea插件激活，亲测可用：\n-------\n#### 在jrebel server处，写上：\n\nhttp://139.199.89.239:1008/88414687-3b91-4286-89ba-2dc813b107ce\n\n如果出现激活过期的情况，请重新生成guid，替换原来的guid即可\n\n邮箱随便写，即可激活。', '2018-07-23 11:02:39', '2018-07-23 11:02:39', 23, 0, 0, 'blog', 0, 1, NULL);
INSERT INTO `blog` VALUES (38, 1, 'vim中 E212：无法打开并写入文件 的解决办法', '<p> 在编写配置文件时，常常忘记切换到root用户，导致文件编辑完毕，敲入:wq 退出保存时，出现 E212：无法打开并写入文件 的错误提示。这是由于在该目录下当前用户没有写权限导致。</p>\n<p>解决办法如下：</p>\n<h5 id=\"h5--1-\"><a name=\"【1】 将文件保存到用户目录下，再改变所有者，然后移动到配置目录下，步骤如下：\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>【1】 将文件保存到用户目录下，再改变所有者，然后移动到配置目录下，步骤如下：</h5><p>1） 保存退出时用 <code>:wq ! ~/tmp/file.conf</code></p>\n<p>2）su切换到root用户，将file.conf改变属主， <code>chown root file.conf</code></p>\n<p>3）移动配置文件到目标目录，<code>mv file.conf  [pathname]</code></p>\n<h5 id=\"h5--2-\"><a name=\"【2】 第二种解决办法相对比较简便\" class=\"reference-link\"></a><span class=\"header-link octicon octicon-link\"></span>【2】 第二种解决办法相对比较简便</h5><p><code>保存文件时用  : w ! sudo tee %</code></p>\n<p><code>tee</code> 用于读取输入文件，同时保存</p>\n<p> %表示当前编辑文件 </p>\n<p>（不过这种方法有个要求就是当前编辑用户必须在 sudoers这个文件中，这也是执行sudo命令的要求</p>\n', ' 在编写配置文件时，常常忘记切换到root用户，导致文件编辑完毕，敲入:wq 退出保存时，出现 E212：无法打开并写入文件 的错误提示。这是由于在该目录下当前用户没有写权限导致。\n\n解决办法如下：\n\n#####【1】 将文件保存到用户目录下，再改变所有者，然后移动到配置目录下，步骤如下：\n\n1） 保存退出时用 `:wq ! ~/tmp/file.conf`\n\n2）su切换到root用户，将file.conf改变属主， `chown root file.conf`\n\n3）移动配置文件到目标目录，`mv file.conf  [pathname]`\n\n#####【2】 第二种解决办法相对比较简便\n`保存文件时用  : w ! sudo tee %`\n\n`tee` 用于读取输入文件，同时保存\n\n %表示当前编辑文件 \n\n（不过这种方法有个要求就是当前编辑用户必须在 sudoers这个文件中，这也是执行sudo命令的要求', '2018-07-25 10:48:58', '2018-07-25 10:48:58', 16, 0, 0, 'blog', 0, 18, NULL);

-- ----------------------------
-- Table structure for blog_category
-- ----------------------------
DROP TABLE IF EXISTS `blog_category`;
CREATE TABLE `blog_category`  (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '名称',
  `status` int(4) NOT NULL DEFAULT 0 COMMENT '是否有效；0是1否',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 10 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '类别表' ROW_FORMAT = Compact;

-- ----------------------------
-- Records of blog_category
-- ----------------------------
INSERT INTO `blog_category` VALUES (1, 'JAVA', 0);
INSERT INTO `blog_category` VALUES (2, 'Python', 0);
INSERT INTO `blog_category` VALUES (3, 'C', 0);
INSERT INTO `blog_category` VALUES (4, 'C++', 0);
INSERT INTO `blog_category` VALUES (5, 'Spring', 0);
INSERT INTO `blog_category` VALUES (6, 'GO', 0);
INSERT INTO `blog_category` VALUES (7, 'JavaScript', 0);
INSERT INTO `blog_category` VALUES (8, 'PHP', 0);
INSERT INTO `blog_category` VALUES (9, 'Kotiln', 0);

-- ----------------------------
-- Table structure for blog_tag
-- ----------------------------
DROP TABLE IF EXISTS `blog_tag`;
CREATE TABLE `blog_tag`  (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '名称',
  `status` int(4) NOT NULL DEFAULT 0 COMMENT '是否有效；0是1否',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 26 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '标签表' ROW_FORMAT = Compact;

-- ----------------------------
-- Records of blog_tag
-- ----------------------------
INSERT INTO `blog_tag` VALUES (1, 'JAVA', 0);
INSERT INTO `blog_tag` VALUES (3, 'JavaScript', 0);
INSERT INTO `blog_tag` VALUES (5, 'Nginx', 0);
INSERT INTO `blog_tag` VALUES (8, 'Mysql', 0);
INSERT INTO `blog_tag` VALUES (9, 'SQL', 0);
INSERT INTO `blog_tag` VALUES (11, '读书笔记', 0);
INSERT INTO `blog_tag` VALUES (12, 'JAVA基础', 0);
INSERT INTO `blog_tag` VALUES (15, 'Tomcat', 0);
INSERT INTO `blog_tag` VALUES (16, '服务器', 0);
INSERT INTO `blog_tag` VALUES (18, 'Linux', 0);
INSERT INTO `blog_tag` VALUES (20, 'Jfinal', 0);
INSERT INTO `blog_tag` VALUES (23, 'SpringBoot', 0);
INSERT INTO `blog_tag` VALUES (24, 'SpringCloud', 0);
INSERT INTO `blog_tag` VALUES (25, 'Redis', 0);

-- ----------------------------
-- Table structure for login_log
-- ----------------------------
DROP TABLE IF EXISTS `login_log`;
CREATE TABLE `login_log`  (
  `accountId` int(11) NOT NULL,
  `loginAt` datetime(0) NOT NULL,
  `ip` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  INDEX `accountId_index`(`accountId`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of login_log
-- ----------------------------
INSERT INTO `login_log` VALUES (1, '2018-06-14 17:44:44', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-14 17:57:45', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-14 17:59:19', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-14 18:03:55', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:06:59', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:17:36', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:24:32', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-14 18:31:49', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-14 22:19:45', '106.87.4.147');
INSERT INTO `login_log` VALUES (1, '2018-06-14 22:26:24', '106.87.4.147');
INSERT INTO `login_log` VALUES (1, '2018-06-14 22:31:25', '106.87.4.147');
INSERT INTO `login_log` VALUES (1, '2018-06-14 22:40:54', '106.87.4.147');
INSERT INTO `login_log` VALUES (1, '2018-06-19 12:36:15', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-19 12:49:11', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-19 13:29:13', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-19 13:56:03', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-19 14:57:13', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-20 10:37:33', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-20 11:10:59', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-20 21:42:44', '106.87.5.52');
INSERT INTO `login_log` VALUES (1, '2018-06-25 19:40:01', '125.82.15.51');
INSERT INTO `login_log` VALUES (1, '2018-06-26 09:18:54', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 09:29:05', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 09:38:11', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 09:55:01', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 09:58:35', '183.64.28.18');
INSERT INTO `login_log` VALUES (2, '2018-06-26 10:48:21', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 11:55:07', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 12:00:31', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-06-26 16:53:04', '47.74.0.27');
INSERT INTO `login_log` VALUES (1, '2018-07-05 08:51:12', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 10:34:16', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 12:47:31', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 17:59:24', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 18:22:25', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 18:23:16', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 18:37:11', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 18:40:21', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-05 21:29:52', '106.87.7.88');
INSERT INTO `login_log` VALUES (1, '2018-07-05 22:04:44', '106.87.7.88');
INSERT INTO `login_log` VALUES (1, '2018-07-05 22:04:44', '106.87.7.88');
INSERT INTO `login_log` VALUES (1, '2018-07-05 22:04:44', '106.87.7.88');
INSERT INTO `login_log` VALUES (1, '2018-07-09 08:51:54', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-09 09:55:42', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-09 10:18:39', '47.74.16.178');
INSERT INTO `login_log` VALUES (1, '2018-07-09 10:46:44', '164.52.13.51');
INSERT INTO `login_log` VALUES (1, '2018-07-09 19:41:46', '47.74.16.178');
INSERT INTO `login_log` VALUES (1, '2018-07-13 09:01:07', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-13 11:09:16', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-13 11:41:52', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-16 11:52:10', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-17 11:10:11', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-18 10:25:19', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-23 11:01:04', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-23 11:09:52', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-23 11:09:52', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-23 11:09:52', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-23 12:58:06', '183.64.28.18');
INSERT INTO `login_log` VALUES (1, '2018-07-25 10:32:43', '45.77.182.136');
INSERT INTO `login_log` VALUES (1, '2018-08-06 16:10:02', '125.84.220.230');
INSERT INTO `login_log` VALUES (1, '2018-09-13 16:11:26', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (1, '2018-09-13 16:18:36', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (1, '2018-09-27 11:06:21', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (1, '2018-09-27 11:29:35', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (3, '2018-09-27 11:31:22', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (3, '2018-09-27 11:50:48', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (1, '2018-09-27 11:56:58', '0:0:0:0:0:0:0:1');
INSERT INTO `login_log` VALUES (3, '2018-09-27 11:57:08', '0:0:0:0:0:0:0:1');

-- ----------------------------
-- Table structure for orders
-- ----------------------------
DROP TABLE IF EXISTS `orders`;
CREATE TABLE `orders`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `account_id` int(11) NULL DEFAULT NULL COMMENT 'account.id',
  `goods_id` int(11) NULL DEFAULT NULL COMMENT '产品id，不同的业务的产品id',
  `scene` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT '订单场景 比如说不同的业务场景，通过此字段来区分',
  `order_sn` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '订单号',
  `pay_status` int(4) NULL DEFAULT 0 COMMENT '支付状态 0-待支付 1-已支付',
  `status` int(4) NOT NULL DEFAULT 0 COMMENT '订单状态 0-待付款 1-待发货 2-待收货 3-待评价 6-退款 7-订单取消 8-订单删除 9-·订单完成',
  `refund_status` int(4) NOT NULL DEFAULT 0 COMMENT '退款状态 0-默认 1-申请中 2-审核中 3-已退款 4-退款失败 5-其他',
  `created` int(11) NULL DEFAULT NULL COMMENT '创建时间',
  `pay_time` int(11) NULL DEFAULT NULL COMMENT '支付时间',
  `send_time` int(11) NULL DEFAULT NULL COMMENT '发货时间',
  `cencel_time` int(11) NULL DEFAULT NULL COMMENT '取消时间',
  `delete_time` int(11) NULL DEFAULT NULL COMMENT '删除时间',
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `order_sn`(`order_sn`) USING BTREE,
  INDEX `scene`(`scene`) USING BTREE,
  INDEX `account_id`(`account_id`) USING BTREE,
  INDEX `goods_id`(`goods_id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Table structure for permission
-- ----------------------------
DROP TABLE IF EXISTS `permission`;
CREATE TABLE `permission`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `actionKey` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '',
  `controller` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '',
  `remark` varchar(1024) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 111 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of permission
-- ----------------------------
INSERT INTO `permission` VALUES (70, '/admin', 'com.choxsu._admin.index.IndexAdminController', '后台首页管理');
INSERT INTO `permission` VALUES (71, '/admin/account', 'com.choxsu._admin.account.AccountAdminController', '账户首页管理');
INSERT INTO `permission` VALUES (72, '/admin/account/addRole', 'com.choxsu._admin.account.AccountAdminController', '增加角色');
INSERT INTO `permission` VALUES (73, '/admin/account/assignRoles', 'com.choxsu._admin.account.AccountAdminController', '分配角色');
INSERT INTO `permission` VALUES (74, '/admin/account/deleteRole', 'com.choxsu._admin.account.AccountAdminController', '删除角色');
INSERT INTO `permission` VALUES (75, '/admin/account/edit', 'com.choxsu._admin.account.AccountAdminController', '账户编辑页面');
INSERT INTO `permission` VALUES (76, '/admin/account/lock', 'com.choxsu._admin.account.AccountAdminController', '账户锁定');
INSERT INTO `permission` VALUES (77, '/admin/account/unlock', 'com.choxsu._admin.account.AccountAdminController', '账户解锁');
INSERT INTO `permission` VALUES (78, '/admin/account/update', 'com.choxsu._admin.account.AccountAdminController', '账户修改');
INSERT INTO `permission` VALUES (80, '/admin/permission', 'com.choxsu._admin.permission.PermissionAdminController', '权限管理首页');
INSERT INTO `permission` VALUES (81, '/admin/permission/delete', 'com.choxsu._admin.permission.PermissionAdminController', '权限删除');
INSERT INTO `permission` VALUES (82, '/admin/permission/edit', 'com.choxsu._admin.permission.PermissionAdminController', '到权限编辑页面');
INSERT INTO `permission` VALUES (83, '/admin/permission/sync', 'com.choxsu._admin.permission.PermissionAdminController', '权限一键同步');
INSERT INTO `permission` VALUES (84, '/admin/permission/update', 'com.choxsu._admin.permission.PermissionAdminController', '更新权限');
INSERT INTO `permission` VALUES (85, '/admin/role', 'com.choxsu._admin.role.RoleAdminController', '角色管理首页');
INSERT INTO `permission` VALUES (86, '/admin/role/add', 'com.choxsu._admin.role.RoleAdminController', '增加角色页面');
INSERT INTO `permission` VALUES (87, '/admin/role/addPermission', 'com.choxsu._admin.role.RoleAdminController', '角色增加权限页面');
INSERT INTO `permission` VALUES (88, '/admin/role/assignPermissions', 'com.choxsu._admin.role.RoleAdminController', '分配权限');
INSERT INTO `permission` VALUES (89, '/admin/role/delete', 'com.choxsu._admin.role.RoleAdminController', '删除角色');
INSERT INTO `permission` VALUES (90, '/admin/role/deletePermission', 'com.choxsu._admin.role.RoleAdminController', '删除角色权限');
INSERT INTO `permission` VALUES (91, '/admin/role/edit', 'com.choxsu._admin.role.RoleAdminController', '到更新角色页面');
INSERT INTO `permission` VALUES (92, '/admin/role/save', 'com.choxsu._admin.role.RoleAdminController', '保存角色');
INSERT INTO `permission` VALUES (93, '/admin/role/update', 'com.choxsu._admin.role.RoleAdminController', '更新角色');
INSERT INTO `permission` VALUES (94, '/admin/blog', 'com.choxsu._admin.blog.AdminBlogController', '博客管理首页');
INSERT INTO `permission` VALUES (95, '/admin/blog/add', 'com.choxsu._admin.blog.AdminBlogController', '到博客添加页面');
INSERT INTO `permission` VALUES (96, '/admin/blog/delete', 'com.choxsu._admin.blog.AdminBlogController', '删除博客');
INSERT INTO `permission` VALUES (97, '/admin/blog/edit', 'com.choxsu._admin.blog.AdminBlogController', '到博客编辑页面');
INSERT INTO `permission` VALUES (98, '/admin/blog/save', 'com.choxsu._admin.blog.AdminBlogController', '博客保存');
INSERT INTO `permission` VALUES (99, '/admin/blog/tag', 'com.choxsu._admin.blog.tag.AdminTagController', '标签管理首页');
INSERT INTO `permission` VALUES (100, '/admin/blog/tag/add', 'com.choxsu._admin.blog.tag.AdminTagController', '到标签添加页面');
INSERT INTO `permission` VALUES (101, '/admin/blog/tag/delete', 'com.choxsu._admin.blog.tag.AdminTagController', '删除标签');
INSERT INTO `permission` VALUES (102, '/admin/blog/tag/edit', 'com.choxsu._admin.blog.tag.AdminTagController', '到标签编辑页面');
INSERT INTO `permission` VALUES (103, '/admin/blog/tag/save', 'com.choxsu._admin.blog.tag.AdminTagController', '保存标签');
INSERT INTO `permission` VALUES (104, '/admin/blog/tag/update', 'com.choxsu._admin.blog.tag.AdminTagController', '更新标签');
INSERT INTO `permission` VALUES (105, '/admin/blog/update', 'com.choxsu._admin.blog.AdminBlogController', '更新博客');
INSERT INTO `permission` VALUES (107, '/admin/visitor', 'com.choxsu._admin.visitor.VisitorAdminController', '访问管理首页');
INSERT INTO `permission` VALUES (108, '/admin/account/add', 'com.choxsu._admin.account.AccountAdminController', NULL);
INSERT INTO `permission` VALUES (109, '/admin/account/del', 'com.choxsu._admin.account.AccountAdminController', NULL);
INSERT INTO `permission` VALUES (110, '/admin/account/save', 'com.choxsu._admin.account.AccountAdminController', NULL);

-- ----------------------------
-- Table structure for role
-- ----------------------------
DROP TABLE IF EXISTS `role`;
CREATE TABLE `role`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '',
  `createAt` datetime(0) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 10 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of role
-- ----------------------------
INSERT INTO `role` VALUES (1, '超级管理员', '2018-03-19 09:58:19');
INSERT INTO `role` VALUES (6, '管理员', '2018-05-03 10:15:44');
INSERT INTO `role` VALUES (8, '小编', '2018-05-03 10:25:52');
INSERT INTO `role` VALUES (9, 'test', '2018-09-27 11:31:48');

-- ----------------------------
-- Table structure for role_permission
-- ----------------------------
DROP TABLE IF EXISTS `role_permission`;
CREATE TABLE `role_permission`  (
  `roleId` int(11) NOT NULL,
  `permissionId` int(11) NOT NULL,
  PRIMARY KEY (`roleId`, `permissionId`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of role_permission
-- ----------------------------
INSERT INTO `role_permission` VALUES (6, 70);
INSERT INTO `role_permission` VALUES (6, 99);
INSERT INTO `role_permission` VALUES (6, 107);
INSERT INTO `role_permission` VALUES (8, 70);
INSERT INTO `role_permission` VALUES (8, 94);
INSERT INTO `role_permission` VALUES (8, 95);
INSERT INTO `role_permission` VALUES (8, 96);
INSERT INTO `role_permission` VALUES (8, 97);
INSERT INTO `role_permission` VALUES (8, 98);
INSERT INTO `role_permission` VALUES (8, 99);
INSERT INTO `role_permission` VALUES (8, 100);
INSERT INTO `role_permission` VALUES (8, 101);
INSERT INTO `role_permission` VALUES (8, 102);
INSERT INTO `role_permission` VALUES (8, 103);
INSERT INTO `role_permission` VALUES (8, 104);
INSERT INTO `role_permission` VALUES (8, 105);
INSERT INTO `role_permission` VALUES (9, 70);
INSERT INTO `role_permission` VALUES (9, 71);
INSERT INTO `role_permission` VALUES (9, 94);
INSERT INTO `role_permission` VALUES (9, 99);

-- ----------------------------
-- Table structure for sensitive_words
-- ----------------------------
DROP TABLE IF EXISTS `sensitive_words`;
CREATE TABLE `sensitive_words`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `word` varchar(32) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '',
  `status` tinyint(4) NOT NULL DEFAULT 1,
  `word_pinyin` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of sensitive_words
-- ----------------------------
INSERT INTO `sensitive_words` VALUES (1, '发票', 1, 'fapiao');

-- ----------------------------
-- Table structure for session
-- ----------------------------
DROP TABLE IF EXISTS `session`;
CREATE TABLE `session`  (
  `id` varchar(33) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `accountId` int(11) NOT NULL,
  `expireAt` bigint(20) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of session
-- ----------------------------
INSERT INTO `session` VALUES ('0975152e537a4199844cad7b3be331de', 1, 1528868094891);
INSERT INTO `session` VALUES ('20e15901ab8b466e858bc0656f4b215b', 1, 1531720330380);
INSERT INTO `session` VALUES ('2e83a52f569446cfbc8e555032d1b7fd', 1, 1529390174632);
INSERT INTO `session` VALUES ('34dd4a9093a64391a35f69a05d60cb61', 1, 1527037041382);
INSERT INTO `session` VALUES ('3908ab2c8fca4db48c4f0a67ca6cc3da', 1, 1526017667329);
INSERT INTO `session` VALUES ('39506ba4fc2c49099f95a44b0a758456', 1, 1528703964628);
INSERT INTO `session` VALUES ('3959f8e4d0a54b5bbcd60fc43e0dd770', 1, 1528977835196);
INSERT INTO `session` VALUES ('3cbf8e1c040549fe9276e7c5e73a371f', 1, 1531804211435);
INSERT INTO `session` VALUES ('430b5da844ce473e9f3f7edd80f05537', 1, 1528734542302);
INSERT INTO `session` VALUES ('44c5310bf2174861bf03be3c3458cb5c', 1, 1531143705817);
INSERT INTO `session` VALUES ('45558eb8e3c944c9ab737e7af45b59ec', 1, 1529984290931);
INSERT INTO `session` VALUES ('4a08190c613349459be84d5b746f9fd8', 1, 1530793395995);
INSERT INTO `session` VALUES ('4d75c58c797240649e36cd0362a42202', 1, 1529509364118);
INSERT INTO `session` VALUES ('597be02bd55d40349667cf9c333dd4b9', 1, 1529301342104);
INSERT INTO `session` VALUES ('5f205bc108d34768b74f8eb2f4d037bf', 1, 1528945032090);
INSERT INTO `session` VALUES ('5fe04df586aa498b8305e42252b68f54', 1, 1619454238258);
INSERT INTO `session` VALUES ('66a1a936e0b74c54867fe8975532b194', 1, 1532493163206);
INSERT INTO `session` VALUES ('73d3fb3683ef4b1da8f3fafb3ff3d7cd', 1, 1529469453206);
INSERT INTO `session` VALUES ('76316c4c2f4248808c6b599c03a1fd1f', 1, 1530791964027);
INSERT INTO `session` VALUES ('769a1fb36a7d43b4bd74f76d6d5f3069', 3, 1538027448261);
INSERT INTO `session` VALUES ('7961183b6e674910835e70efd9b8af94', 1, 1525251405079);
INSERT INTO `session` VALUES ('83ed11787d5a403097cc0bf4d9759864', 1, 1536833916433);
INSERT INTO `session` VALUES ('84c53635fc67483a9809148c0647a8a8', 1, 1525242085682);
INSERT INTO `session` VALUES ('8790a908a022489980c2f0b3afd07291', 1, 1531108542002);
INSERT INTO `session` VALUES ('892dfe57faf94aa3900eb294ca37f110', 1, 1531104714187);
INSERT INTO `session` VALUES ('946ae77da0284a6ebb338778fe5b7888', 1, 1528734499924);
INSERT INTO `session` VALUES ('9c90a3865bab4db79cb08a2f79b3a8f3', 1, 1529301059326);
INSERT INTO `session` VALUES ('a30005607a2e4030a6a701721c93e762', 1, 1538024781378);
INSERT INTO `session` VALUES ('a4f8f4c9d1974aa9ac311cf4e73ff1e6', 1, 1530804592100);
INSERT INTO `session` VALUES ('a990e8dac11243979f4968e4a152448c', 1, 1531887919018);
INSERT INTO `session` VALUES ('aa583a75b4cf432793fd62ce21a7cc85', 3, 1538026281880);
INSERT INTO `session` VALUES ('af3d6ab0c9f1446db3a44bdcf1ce147f', 9, 1525321633446);
INSERT INTO `session` VALUES ('b6e786a752a24c868ef76a5d9d7ab908', 1, 1530010383610);
INSERT INTO `session` VALUES ('bb9f91bdf48d4ffba0669c175a40480e', 1, 1528977558572);
INSERT INTO `session` VALUES ('bc8fb990ffab4ef9a15a15dd2dfd2969', 1, 1529934000549);
INSERT INTO `session` VALUES ('be7ddede69da47ac95f99aeb672c2f05', 1, 1529398632560);
INSERT INTO `session` VALUES ('c2ed4d3b98814a3ea86259511b0fc523', 1, 1528425292500);
INSERT INTO `session` VALUES ('e0a58e1a8a3b4dcf93db37093296d3e7', 1, 1531458555567);
INSERT INTO `session` VALUES ('e9b5712ee61d47368cc79000354bb995', 1, 1528993184577);
INSERT INTO `session` VALUES ('f1180527572a4a1e872f0e7b98848344', 1, 1528968843645);
INSERT INTO `session` VALUES ('f538e022c9134baea27f1c5a5374cf4e', 1, 1528951222466);

-- ----------------------------
-- Table structure for visitor
-- ----------------------------
DROP TABLE IF EXISTS `visitor`;
CREATE TABLE `visitor`  (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `ip` varchar(225) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '请求的IP地址',
  `url` varchar(336) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '请求的页面路径',
  `method` varchar(225) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '请求方法',
  `client` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL,
  `requestTime` datetime(0) NULL DEFAULT NULL COMMENT '请求时间',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 24845 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of visitor
-- ----------------------------
SET FOREIGN_KEY_CHECKS = 1;
